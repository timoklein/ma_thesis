
@article{howardSearchingMobileNetV32019,
	title = {Searching for {MobileNetV3}},
	url = {http://arxiv.org/abs/1905.02244},
	abstract = {We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardwareaware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efﬁcient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classiﬁcation, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classiﬁcation while reducing latency by 20\% compared to MobileNetV2. MobileNetV3-Small is 6.6\% more accurate compared to a MobileNetV2 model with comparable latency. MobileNetV3-Large detection is over 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LRASPP is 34\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.},
	language = {en},
	urldate = {2020-11-13},
	journal = {arXiv:1905.02244 [cs]},
	author = {Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and Le, Quoc V. and Adam, Hartwig},
	month = nov,
	year = {2019},
	note = {arXiv: 1905.02244},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Howard et al. - 2019 - Searching for MobileNetV3.pdf:/home/timo/Zotero/storage/ZT63PH94/Howard et al. - 2019 - Searching for MobileNetV3.pdf:application/pdf},
}

@book{oliehoekConciseIntroductionDecentralized2016,
	address = {Cham},
	series = {{SpringerBriefs} in {Intelligent} {Systems}},
	title = {A {Concise} {Introduction} to {Decentralized} {POMDPs}},
	isbn = {978-3-319-28927-4 978-3-319-28929-8},
	url = {http://link.springer.com/10.1007/978-3-319-28929-8},
	language = {en},
	urldate = {2020-11-13},
	publisher = {Springer International Publishing},
	author = {Oliehoek, Frans A. and Amato, Christopher},
	year = {2016},
	doi = {10.1007/978-3-319-28929-8},
	file = {Oliehoek and Amato - 2016 - A Concise Introduction to Decentralized POMDPs.pdf:/home/timo/Zotero/storage/434RACCS/Oliehoek and Amato - 2016 - A Concise Introduction to Decentralized POMDPs.pdf:application/pdf},
}

@article{wangECANetEfficientChannel2020,
	title = {{ECA}-{Net}: {Efficient} {Channel} {Attention} for {Deep} {Convolutional} {Neural} {Networks}},
	shorttitle = {{ECA}-{Net}},
	url = {http://arxiv.org/abs/1910.03151},
	abstract = {Recently, channel attention mechanism has demonstrated to offer great potential in improving the performance of deep convolutional neural networks (CNNs). However, most existing methods dedicate to developing more sophisticated attention modules for achieving better performance, which inevitably increase model complexity. To overcome the paradox of performance and complexity trade-off, this paper proposes an Efﬁcient Channel Attention (ECA) module, which only involves a handful of parameters while bringing clear performance gain. By dissecting the channel attention module in SENet, we empirically show avoiding dimensionality reduction is important for learning channel attention, and appropriate cross-channel interaction can preserve performance while signiﬁcantly decreasing model complexity. Therefore, we propose a local crosschannel interaction strategy without dimensionality reduction, which can be efﬁciently implemented via 1D convolution. Furthermore, we develop a method to adaptively select kernel size of 1D convolution, determining coverage of local cross-channel interaction. The proposed ECA module is efﬁcient yet effective, e.g., the parameters and computations of our modules against backbone of ResNet50 are 80 vs. 24.37M and 4.7e-4 GFLOPs vs. 3.86 GFLOPs, respectively, and the performance boost is more than 2\% in terms of Top-1 accuracy. We extensively evaluate our ECA module on image classiﬁcation, object detection and instance segmentation with backbones of ResNets and MobileNetV2. The experimental results show our module is more efﬁcient while performing favorably against its counterparts.},
	language = {en},
	urldate = {2020-11-12},
	journal = {arXiv:1910.03151 [cs]},
	author = {Wang, Qilong and Wu, Banggu and Zhu, Pengfei and Li, Peihua and Zuo, Wangmeng and Hu, Qinghua},
	month = apr,
	year = {2020},
	note = {arXiv: 1910.03151},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Wang et al. - 2020 - ECA-Net Efficient Channel Attention for Deep Conv.pdf:/home/timo/Zotero/storage/JXSVNBGY/Wang et al. - 2020 - ECA-Net Efficient Channel Attention for Deep Conv.pdf:application/pdf},
}

@article{ioffeBatchNormalizationAccelerating2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classiﬁcation model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a signiﬁcant margin. Using an ensemble of batchnormalized networks, we improve upon the best published result on ImageNet classiﬁcation: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	language = {en},
	urldate = {2020-11-12},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Machine Learning},
	file = {Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:/home/timo/Zotero/storage/XYNPECFP/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf},
}

@article{bacchianiMicroscopicTrafficSimulation2019,
	title = {Microscopic {Traffic} {Simulation} by {Cooperative} {Multi}-agent {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1903.01365},
	abstract = {Expert human drivers perform actions relying on traffic laws and their previous experience. While traffic laws are easily embedded into an artificial brain, modeling human complex behaviors which come from past experience is a more challenging task. One of these behaviors is the capability of communicating intentions and negotiating the right of way through driving actions, as when a driver is entering a crowded roundabout and observes other cars movements to guess the best time to merge in. In addition, each driver has its own unique driving style, which is conditioned by both its personal characteristics, such as age and quality of sight, and external factors, such as being late or in a bad mood. For these reasons, the interaction between different drivers is not trivial to simulate in a realistic manner. In this paper, this problem is addressed by developing a microscopic simulator using a Deep Reinforcement Learning Algorithm based on a combination of visual frames, representing the perception around the vehicle, and a vector of numerical parameters. In particular, the algorithm called Asynchronous Advantage Actor-Critic has been extended to a multi-agent scenario in which every agent needs to learn to interact with other similar agents. Moreover, the model includes a novel architecture such that the driving style of each vehicle is adjustable by tuning some of its input parameters, permitting to simulate drivers with different levels of aggressiveness and desired cruising speeds.},
	language = {en},
	urldate = {2020-11-10},
	journal = {arXiv:1903.01365 [cs]},
	author = {Bacchiani, Giulio and Molinari, Daniele and Patander, Marco},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.01365},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	file = {Bacchiani et al. - 2019 - Microscopic Traffic Simulation by Cooperative Mult.pdf:/home/timo/Zotero/storage/7MFRIZDX/Bacchiani et al. - 2019 - Microscopic Traffic Simulation by Cooperative Mult.pdf:application/pdf},
}

@article{loweMultiAgentActorCriticMixed2020,
	title = {Multi-{Agent} {Actor}-{Critic} for {Mixed} {Cooperative}-{Competitive} {Environments}},
	url = {http://arxiv.org/abs/1706.02275},
	abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difﬁculty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multiagent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
	language = {en},
	urldate = {2020-11-09},
	journal = {arXiv:1706.02275 [cs]},
	author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
	month = mar,
	year = {2020},
	note = {arXiv: 1706.02275},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
	file = {Lowe et al. - 2020 - Multi-Agent Actor-Critic for Mixed Cooperative-Com.pdf:/home/timo/Zotero/storage/VTWT8T23/Lowe et al. - 2020 - Multi-Agent Actor-Critic for Mixed Cooperative-Com.pdf:application/pdf},
}

@article{rolnickTacklingClimateChange2019,
	title = {Tackling {Climate} {Change} with {Machine} {Learning}},
	url = {http://arxiv.org/abs/1906.05433},
	abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be ﬁlled by machine learning, in collaboration with other ﬁelds. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.},
	language = {en},
	urldate = {2020-11-09},
	journal = {arXiv:1906.05433 [cs, stat]},
	author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
	month = nov,
	year = {2019},
	note = {arXiv: 1906.05433},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Computers and Society},
	file = {Rolnick et al. - 2019 - Tackling Climate Change with Machine Learning.pdf:/home/timo/Zotero/storage/BWM3NJQU/Rolnick et al. - 2019 - Tackling Climate Change with Machine Learning.pdf:application/pdf},
}

@article{brownLanguageModelsAre2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic in architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁnetuning approaches. Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning, with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we ﬁnd that GPT-3 can generate samples of news articles which human evaluators have difﬁculty distinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding and of GPT-3 in general.},
	language = {en},
	urldate = {2020-11-07},
	journal = {arXiv:2005.14165 [cs]},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv: 2005.14165},
	keywords = {Computer Science - Computation and Language},
	file = {Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:/home/timo/Zotero/storage/KPWL85KN/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@article{schulmanTrustRegionPolicy2017,
	title = {Trust {Region} {Policy} {Optimization}},
	url = {http://arxiv.org/abs/1502.05477},
	abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justiﬁed procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:1502.05477 [cs]},
	author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
	month = apr,
	year = {2017},
	note = {arXiv: 1502.05477},
	keywords = {Computer Science - Machine Learning},
	file = {Schulman et al. - 2017 - Trust Region Policy Optimization.pdf:/home/timo/Zotero/storage/RKSND2K7/Schulman et al. - 2017 - Trust Region Policy Optimization.pdf:application/pdf},
}

@article{kurzerAcceleratingCooperativePlanning2020,
	title = {Accelerating {Cooperative} {Planning} for {Automated} {Vehicles} with {Learned} {Heuristics} and {Monte} {Carlo} {Tree} {Search}},
	url = {http://arxiv.org/abs/2002.00497},
	abstract = {Efﬁcient driving in urban trafﬁc scenarios requires foresight. The observation of other trafﬁc participants and the inference of their possible next actions depending on the own action is considered cooperative prediction and planning. Humans are well equipped with the capability to predict the actions of multiple interacting trafﬁc participants and plan accordingly, without the need to directly communicate with others. Prior work has shown that it is possible to achieve effective cooperative planning without the need for explicit communication. However, the search space for cooperative plans is so large that most of the computational budget is spent on exploring the search space in unpromising regions that are far away from the solution. To accelerate the planning process, we combined learned heuristics with a cooperative planning method to guide the search towards regions with promising actions, yielding better solutions at lower computational costs.},
	language = {en},
	urldate = {2020-11-04},
	journal = {arXiv:2002.00497 [cs, stat]},
	author = {Kurzer, Karl and Fechner, Marcus and Zöllner, J. Marius},
	month = may,
	year = {2020},
	note = {arXiv: 2002.00497},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning, Computer Science - Multiagent Systems},
	file = {Kurzer et al. - 2020 - Accelerating Cooperative Planning for Automated Ve.pdf:/home/timo/Zotero/storage/ZHBQ9XGT/Kurzer et al. - 2020 - Accelerating Cooperative Planning for Automated Ve.pdf:application/pdf},
}

@article{kurzerDecentralizedCooperativePlanning2018,
	title = {Decentralized {Cooperative} {Planning} for {Automated} {Vehicles} with {Continuous} {Monte} {Carlo} {Tree} {Search}},
	url = {http://arxiv.org/abs/1809.03200},
	doi = {10.1109/ITSC.2018.8569988},
	abstract = {Urban trafﬁc scenarios often require a high degree of cooperation between trafﬁc participants to ensure safety and efﬁciency. Observing the behavior of others, humans infer whether or not others are cooperating. This work aims to extend the capabilities of automated vehicles, enabling them to cooperate implicitly in heterogeneous environments. Continuous actions allow for arbitrary trajectories and hence are applicable to a much wider class of problems than existing cooperative approaches with discrete action spaces. Based on cooperative modeling of other agents, Monte Carlo Tree Search (MCTS) in conjunction with Decoupled-UCT evaluates the action-values of each agent in a cooperative and decentralized way, respecting the interdependence of actions among trafﬁc participants. The extension to continuous action spaces is addressed by incorporating novel MCTS-speciﬁc enhancements for efﬁcient search space exploration. The proposed algorithm is evaluated under different scenarios, showing that the algorithm is able to achieve effective cooperative planning and generate solutions egocentric planning fails to identify.},
	language = {en},
	urldate = {2020-11-04},
	journal = {2018 21st International Conference on Intelligent Transportation Systems (ITSC)},
	author = {Kurzer, Karl and Engelhorn, Florian and Zöllner, J. Marius},
	month = nov,
	year = {2018},
	note = {arXiv: 1809.03200},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	pages = {452--459},
	file = {Kurzer et al. - 2018 - Decentralized Cooperative Planning for Automated V.pdf:/home/timo/Zotero/storage/N55MYS24/Kurzer et al. - 2018 - Decentralized Cooperative Planning for Automated V.pdf:application/pdf},
}

@article{hendersonDeepReinforcementLearning2019,
	title = {Deep {Reinforcement} {Learning} that {Matters}},
	url = {http://arxiv.org/abs/1709.06560},
	abstract = {In recent years, signiﬁcant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without signiﬁcance metrics and tighter standardization of experimental reporting, it is difﬁcult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the ﬁeld by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
	language = {en},
	urldate = {2020-08-18},
	journal = {arXiv:1709.06560 [cs, stat]},
	author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
	month = jan,
	year = {2019},
	note = {arXiv: 1709.06560},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Henderson et al. - 2019 - Deep Reinforcement Learning that Matters.pdf:/home/timo/Zotero/storage/FRYFSXX9/Henderson et al. - 2019 - Deep Reinforcement Learning that Matters.pdf:application/pdf},
}

@article{wrightNeuralAttentionalArchitecturesDeep,
	title = {Neural-{Attentional} {Architectures} for {Deep} {Multi}-{Agent} {Reinforcement} {Learning} in {Varying} {Environments}},
	abstract = {Many potential applications of reinforcement learning (RL) in the real world involve interacting with other agents whose numbers vary over time. We propose new neural architectures for these multi-agent RL problems. In contrast to other methods of training an individual, discrete policy for each agent and then enforcing cooperation through some additional inter-policy mechanism, we propose learning multi-agent relationships at the policy level by using an attentional architecture. In our method, all agents share the same policy, but independently apply it in their own context to aggregate the other agents’ state information when selecting their next action. The structure of our architectures allow them to be applied on environments with varying numbers of agents. We demonstrate our architecture on a benchmark multi-agent autonomous vehicle coordination problem, obtaining superior results to a full-knowledge, fully-centralized reference solution, and signiﬁcantly outperforming it when scaling to large numbers of agents.},
	language = {en},
	author = {Wright, Matthew A and Horowitz, Roberto},
	pages = {8},
	file = {Wright and Horowitz - Neural-Attentional Architectures for Deep Multi-Ag.pdf:/home/timo/Zotero/storage/EGJQQZCJ/Wright and Horowitz - Neural-Attentional Architectures for Deep Multi-Ag.pdf:application/pdf},
}

@article{tangDiscretizingContinuousAction2020,
	title = {Discretizing {Continuous} {Action} {Space} for {On}-{Policy} {Optimization}},
	url = {http://arxiv.org/abs/1901.10500},
	abstract = {In this work, we show that discretizing action space for continuous control is a simple yet powerful technique for on-policy optimization. The explosion in the number of discrete actions can be efﬁciently addressed by a policy with factorized distribution across action dimensions. We show that the discrete policy achieves signiﬁcant performance gains with state-of-the-art on-policy optimization algorithms (PPO, TRPO, ACKTR) especially on high-dimensional tasks with complex dynamics. Additionally, we show that an ordinal parameterization of the discrete distribution can introduce the inductive bias that encodes the natural ordering between discrete actions. This ordinal architecture further signiﬁcantly improves the performance of PPO/TRPO. An open source implementation of this paper can be found at https://github. com/robintyh1/onpolicybaselines.},
	language = {en},
	urldate = {2020-08-04},
	journal = {arXiv:1901.10500 [cs]},
	author = {Tang, Yunhao and Agrawal, Shipra},
	month = mar,
	year = {2020},
	note = {arXiv: 1901.10500},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Tang and Agrawal - 2020 - Discretizing Continuous Action Space for On-Policy.pdf:/home/timo/Zotero/storage/QC2UEKMD/Tang and Agrawal - 2020 - Discretizing Continuous Action Space for On-Policy.pdf:application/pdf},
}

@article{everettMotionPlanningDynamic2018,
	title = {Motion {Planning} {Among} {Dynamic}, {Decision}-{Making} {Agents} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1805.01956},
	abstract = {Robots that navigate among pedestrians use collision avoidance algorithms to enable safe and efﬁcient operation. Recent works present deep reinforcement learning as a framework to model the complex interactions and cooperation. However, they are implemented using key assumptions about other agents’ behavior that deviate from reality as the number of agents in the environment increases. This work extends our previous approach to develop an algorithm that learns collision avoidance among a variety of types of dynamic agents without assuming they follow any particular behavior rules. This work also introduces a strategy using LSTM that enables the algorithm to use observations of an arbitrary number of other agents, instead of previous methods that have a ﬁxed observation size. The proposed algorithm outperforms our previous approach in simulation as the number of agents increases, and the algorithm is demonstrated on a fully autonomous robotic vehicle traveling at human walking speed, without the use of a 3D Lidar.},
	language = {en},
	urldate = {2020-07-15},
	journal = {arXiv:1805.01956 [cs]},
	author = {Everett, Michael and Chen, Yu Fan and How, Jonathan P.},
	month = may,
	year = {2018},
	note = {arXiv: 1805.01956},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {Everett et al. - 2018 - Motion Planning Among Dynamic, Decision-Making Age.pdf:/home/timo/Zotero/storage/Z4M5DNC2/Everett et al. - 2018 - Motion Planning Among Dynamic, Decision-Making Age.pdf:application/pdf},
}

@article{liuWATCHUNOBSERVEDSIMPLE2020,
	title = {{WATCH} {THE} {UNOBSERVED}: {A} {SIMPLE} {APPROACH} {TO} {PARALLELIZING} {MONTE} {CARLO} {TREE} {SEARCH}},
	abstract = {Monte Carlo Tree Search (MCTS) algorithms have achieved great success on many challenging benchmarks (e.g., Computer Go). However, they generally require a large number of rollouts, making their applications costly. Furthermore, it is also extremely challenging to parallelize MCTS due to its inherent sequential nature: each rollout heavily relies on the statistics (e.g., node visitation counts) estimated from previous simulations to achieve an effective exploration-exploitation tradeoff. In spite of these difﬁculties, we develop an algorithm, WU-UCT1, to effectively parallelize MCTS, which achieves linear speedup and exhibits only limited performance loss with an increasing number of workers. The key idea in WU-UCT is a set of statistics that we introduce to track the number of on-going yet incomplete simulation queries (named as unobserved samples). These statistics are used to modify the UCT tree policy in the selection steps in a principled manner to retain effective exploration-exploitation tradeoff when we parallelize the most time-consuming expansion and simulation steps. Experiments on a proprietary benchmark and the Atari Game benchmark demonstrate the linear speedup and the superior performance of WU-UCT comparing to existing techniques.},
	language = {en},
	author = {Liu, Anji and Chen, Jianshu and Yu, Mingze and Zhai, Yu and Zhou, Xuewen and Liu, Ji},
	year = {2020},
	pages = {21},
	file = {Liu et al. - 2020 - WATCH THE UNOBSERVED A SIMPLE APPROACH TO PARALLE.pdf:/home/timo/Zotero/storage/CBTRUM8T/Liu et al. - 2020 - WATCH THE UNOBSERVED A SIMPLE APPROACH TO PARALLE.pdf:application/pdf},
}

@article{heDeepResidualLearning2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2020-07-14},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:/home/timo/Zotero/storage/2JRYTVIX/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@article{willemsenValueTargetsOffpolicy,
	title = {Value targets in off-policy {AlphaZero}: a new greedy backup},
	abstract = {This article presents and evaluates a family of AlphaZero value targets, subsuming previous variants and introducing AlphaZero with greedy backups (A0GB). Current state-of-the-art algorithms for playing board games use sample-based planning, such as Monte Carlo Tree Search (MCTS), combined with deep neural networks (NN) to approximate the value function. These algorithms, of which AlphaZero is a prominent example, are computationally extremely expensive to train, due to their reliance on many neural network evaluations. This limits their practical performance. We improve the training process of AlphaZero by using more effective training targets for the neural network. We introduce a three-dimensional space to describe a family of training targets, covering the original AlphaZero training target as well as the soft-Z and A0C variants from the literature. We demonstrate that A0GB, using a specific new value target from this family, is able to find the optimal policy in a small tabular domain, whereas the original AlphaZero target fails to do so. In addition, we show that soft-Z, A0C and A0GB achieve better performance and faster training than the original AlphaZero target on two benchmark board games (Connect-Four and Breakthrough).},
	language = {en},
	author = {Willemsen, Daniel and Baier, Hendrik and Kaisers, Michael},
	pages = {9},
	file = {Willemsen et al. - Value targets in off-policy AlphaZero a new greed.pdf:/home/timo/Zotero/storage/6GGLIHKS/Willemsen et al. - Value targets in off-policy AlphaZero a new greed.pdf:application/pdf},
}

@article{yangContinuousControlSearching2020,
	title = {Continuous {Control} for {Searching} and {Planning} with a {Learned} {Model}},
	url = {http://arxiv.org/abs/2006.07430},
	abstract = {Decision-making agents with planning capabilities have achieved huge success in the challenging domain like Chess, Shogi, and Go. In an effort to generalize the planning ability to the more general tasks where the environment dynamics are not available to the agent, researchers proposed the MuZero algorithm that can learn the dynamical model through the interactions with the environment. In this paper, we provide a way and the necessary theoretical results to extend the MuZero algorithm to more generalized environments with continuous action space. Through numerical results on two relatively low-dimensional MuJoCo environments, we show the proposed algorithm outperforms the soft actor-critic (SAC) algorithm, a state-of-the-art model-free deep reinforcement learning algorithm.},
	language = {en},
	urldate = {2020-07-04},
	journal = {arXiv:2006.07430 [cs]},
	author = {Yang, Xuxi and Duvaud, Werner and Wei, Peng},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.07430},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Yang et al. - 2020 - Continuous Control for Searching and Planning with.pdf:/home/timo/Zotero/storage/DDHJLQP3/Yang et al. - 2020 - Continuous Control for Searching and Planning with.pdf:application/pdf},
}

@article{jaderbergHumanlevelPerformance3D2019,
	title = {Human-level performance in {3D} multiplayer games with population-based reinforcement learning},
	volume = {364},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aau6249},
	doi = {10.1126/science.aau6249},
	abstract = {Reinforcement learning (RL) has shown great success in increasingly complex single-agent environments and two-player turn-based games. However, the real world contains multiple agents, each learning and acting independently to cooperate and compete with other agents. We used a tournament-style evaluation to demonstrate that an agent can achieve human-level performance in a three-dimensional multiplayer first-person video game,
              Quake III Arena
              in Capture the Flag mode, using only pixels and game points scored as input. We used a two-tier optimization process in which a population of independent RL agents are trained concurrently from thousands of parallel matches on randomly generated environments. Each agent learns its own internal reward signal and rich representation of the world. These results indicate the great potential of multiagent reinforcement learning for artificial intelligence research.},
	language = {en},
	number = {6443},
	urldate = {2020-07-04},
	journal = {Science},
	author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Castañeda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
	month = may,
	year = {2019},
	pages = {859--865},
	file = {Jaderberg et al. - 2019 - Human-level performance in 3D multiplayer games wi.pdf:/home/timo/Zotero/storage/KH828MFK/Jaderberg et al. - 2019 - Human-level performance in 3D multiplayer games wi.pdf:application/pdf},
}

@article{huSqueezeandExcitationNetworks2019,
	title = {Squeeze-and-{Excitation} {Networks}},
	url = {http://arxiv.org/abs/1709.01507},
	abstract = {The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive ﬁelds at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring signiﬁcant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classiﬁcation submission which won ﬁrst place and reduced the top-5 error to 2.251\%, surpassing the winning entry of 2016 by a relative improvement of ∼25\%. Models and code are available at https://github.com/hujie-frank/SENet.},
	language = {en},
	urldate = {2020-07-04},
	journal = {arXiv:1709.01507 [cs]},
	author = {Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Wu, Enhua},
	month = may,
	year = {2019},
	note = {arXiv: 1709.01507},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Hu et al. - 2019 - Squeeze-and-Excitation Networks.pdf:/home/timo/Zotero/storage/8HNX5K9Q/Hu et al. - 2019 - Squeeze-and-Excitation Networks.pdf:application/pdf},
}

@article{wuAcceleratingSelfPlayLearning2020,
	title = {Accelerating {Self}-{Play} {Learning} in {Go}},
	url = {http://arxiv.org/abs/1902.10565},
	abstract = {By introducing several improvements to the AlphaZero process and architecture, we greatly accelerate self-play learning in Go, achieving a 50x reduction in computation over comparable methods. Like AlphaZero and replications such as ELF OpenGo and Leela Zero, our bot KataGo only learns from neural-net-guided Monte Carlo tree search self-play. But whereas AlphaZero required thousands of TPUs over several days and ELF required thousands of GPUs over two weeks, KataGo surpasses ELF’s ﬁnal model after only 19 days on fewer than 30 GPUs. Much of the speedup involves non-domain-speciﬁc improvements that might directly transfer to other problems. Further gains from domain-speciﬁc techniques reveal the remaining eﬃciency gap between the best methods and purely general methods such as AlphaZero. Our work is a step towards making learning in state spaces as large as Go possible without large-scale computational resources.},
	language = {en},
	urldate = {2020-07-03},
	journal = {arXiv:1902.10565 [cs, stat]},
	author = {Wu, David J.},
	month = feb,
	year = {2020},
	note = {arXiv: 1902.10565},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Wu - 2020 - Accelerating Self-Play Learning in Go.pdf:/home/timo/Zotero/storage/5N6SQJRZ/Wu - 2020 - Accelerating Self-Play Learning in Go.pdf:application/pdf},
}

@article{laskinReinforcementLearningAugmented2020,
	title = {Reinforcement {Learning} with {Augmented} {Data}},
	url = {http://arxiv.org/abs/2004.14990},
	abstract = {Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efﬁciency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the ﬁrst extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efﬁciency and ﬁnal performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD signiﬁcantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks. Our RAD module and training code are available at https://www.github.com/MishaLaskin/rad.},
	language = {en},
	urldate = {2020-07-02},
	journal = {arXiv:2004.14990 [cs, stat]},
	author = {Laskin, Michael and Lee, Kimin and Stooke, Adam and Pinto, Lerrel and Abbeel, Pieter and Srinivas, Aravind},
	month = jun,
	year = {2020},
	note = {arXiv: 2004.14990},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Laskin et al. - 2020 - Reinforcement Learning with Augmented Data.pdf:/home/timo/Zotero/storage/YN8PKJD6/Laskin et al. - 2020 - Reinforcement Learning with Augmented Data.pdf:application/pdf},
}

@article{tianELFOpenGoAnalysis2019,
	title = {{ELF} {OpenGo}: {An} {Analysis} and {Open} {Reimplementation} of {AlphaZero}},
	shorttitle = {{ELF} {OpenGo}},
	url = {http://arxiv.org/abs/1902.04522},
	abstract = {The AlphaGo, AlphaGo Zero, and AlphaZero series of algorithms are remarkable demonstrations of deep reinforcement learning's capabilities, achieving superhuman performance in the complex game of Go with progressively increasing autonomy. However, many obstacles remain in the understanding of and usability of these promising approaches by the research community. Toward elucidating unresolved mysteries and facilitating future research, we propose ELF OpenGo, an open-source reimplementation of the AlphaZero algorithm. ELF OpenGo is the first open-source Go AI to convincingly demonstrate superhuman performance with a perfect (20:0) record against global top professionals. We apply ELF OpenGo to conduct extensive ablation studies, and to identify and analyze numerous interesting phenomena in both the model training and in the gameplay inference procedures. Our code, models, selfplay datasets, and auxiliary data are publicly available.},
	language = {en},
	urldate = {2020-07-02},
	journal = {arXiv:1902.04522 [cs, stat]},
	author = {Tian, Yuandong and Ma, Jerry and Gong, Qucheng and Sengupta, Shubho and Chen, Zhuoyuan and Pinkerton, James and Zitnick, C. Lawrence},
	month = may,
	year = {2019},
	note = {arXiv: 1902.04522},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Tian et al. - 2019 - ELF OpenGo An Analysis and Open Reimplementation .pdf:/home/timo/Zotero/storage/LEGJI4ZB/Tian et al. - 2019 - ELF OpenGo An Analysis and Open Reimplementation .pdf:application/pdf},
}

@article{wuAcceleratingImprovingAlphaZero2020,
	title = {Accelerating and {Improving} {AlphaZero} {Using} {Population} {Based} {Training}},
	url = {http://arxiv.org/abs/2003.06212},
	abstract = {AlphaZero has been very successful in many games. Unfortunately, it still consumes a huge amount of computing resources, the majority of which is spent in self-play. Hyperparameter tuning exacerbates the training cost since each hyperparameter conﬁguration requires its own time to train one run, during which it will generate its own self-play records. As a result, multiple runs are usually needed for different hyperparameter conﬁgurations. This paper proposes using population based training (PBT) to help tune hyperparameters dynamically and improve strength during training time. Another signiﬁcant advantage is that this method requires a single run only, while incurring a small additional time cost, since the time for generating self-play records remains unchanged though the time for optimization is increased following the AlphaZero training algorithm. In our experiments for 9x9 Go, the PBT method is able to achieve a higher win rate for 9x9 Go than the baselines, each with its own hyperparameter conﬁguration and trained individually. For 19x19 Go, with PBT, we are able to obtain improvements in playing strength. Speciﬁcally, the PBT agent can obtain up to 74\% win rate against ELF OpenGo, an open-source state-of-theart AlphaZero program using a neural network of a comparable capacity. This is compared to a saturated non-PBT agent, which achieves a win rate of 47\% against ELF OpenGo under the same circumstances.},
	language = {en},
	urldate = {2020-07-02},
	journal = {arXiv:2003.06212 [cs]},
	author = {Wu, Ti-Rong and Wei, Ting-Han and Wu, I.-Chen},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.06212},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	file = {Wu et al. - 2020 - Accelerating and Improving AlphaZero Using Populat.pdf:/home/timo/Zotero/storage/TNSLWYNX/Wu et al. - 2020 - Accelerating and Improving AlphaZero Using Populat.pdf:application/pdf},
}

@article{silverGeneralReinforcementLearning2018,
	title = {A general reinforcement learning algorithm that masters chess, shogi, and {Go} through self-play},
	volume = {362},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aar6404},
	doi = {10.1126/science.aar6404},
	abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
	language = {en},
	number = {6419},
	urldate = {2020-06-25},
	journal = {Science},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	month = dec,
	year = {2018},
	pages = {1140--1144},
	file = {Silver et al. - 2018 - A general reinforcement learning algorithm that ma.pdf:/home/timo/Zotero/storage/GVELQQWQ/Silver et al. - 2018 - A general reinforcement learning algorithm that ma.pdf:application/pdf},
}

@article{anthonyThinkingFastSlow2017,
	title = {Thinking {Fast} and {Slow} with {Deep} {Learning} and {Tree} {Search}},
	url = {http://arxiv.org/abs/1705.08439},
	abstract = {Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (EXIT), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that EXIT outperforms REINFORCE for training a neural network to play the board game Hex, and our ﬁnal tree search agent, trained tabula rasa, defeats MOHEX 1.0, the most recent Olympiad Champion player to be publicly released.},
	language = {en},
	urldate = {2020-06-24},
	journal = {arXiv:1705.08439 [cs]},
	author = {Anthony, Thomas and Tian, Zheng and Barber, David},
	month = dec,
	year = {2017},
	note = {arXiv: 1705.08439},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Anthony et al. - 2017 - Thinking Fast and Slow with Deep Learning and Tree.pdf:/home/timo/Zotero/storage/G3KID8E6/Anthony et al. - 2017 - Thinking Fast and Slow with Deep Learning and Tree.pdf:application/pdf},
}

@article{wangHyperParameterSweepAlphaZero2019,
	title = {Hyper-{Parameter} {Sweep} on {AlphaZero} {General}},
	url = {http://arxiv.org/abs/1903.08129},
	abstract = {Since AlphaGo and AlphaGo Zero have achieved breakground successes in the game of Go, the programs have been generalized to solve other tasks. Subsequently, AlphaZero was developed to play Go, Chess and Shogi. In the literature, the algorithms are explained well. However, AlphaZero contains many parameters, and for neither AlphaGo, AlphaGo Zero nor AlphaZero, there is suﬃcient discussion about how to set parameter values in these algorithms. Therefore, in this paper, we choose 12 parameters in AlphaZero and evaluate how these parameters contribute to training. We focus on three objectives (training loss, time cost and playing strength). For each parameter, we train 3 models using 3 diﬀerent values (minimum value, default value, maximum value). We use the game of play 6×6 Othello, on the AlphaZeroGeneral open source re-implementation of AlphaZero. Overall, experimental results show that diﬀerent values can lead to diﬀerent training results, proving the importance of such a parameter sweep. We categorize these 12 parameters into time-sensitive parameters and time-friendly parameters. Moreover, through multi-objective analysis, this paper provides an insightful basis for further hyper-parameter optimization.},
	language = {en},
	urldate = {2020-06-24},
	journal = {arXiv:1903.08129 [cs]},
	author = {Wang, Hui and Emmerich, Michael and Preuss, Mike and Plaat, Aske},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.08129},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Wang et al. - 2019 - Hyper-Parameter Sweep on AlphaZero General.pdf:/home/timo/Zotero/storage/NW6A4TZ2/Wang et al. - 2019 - Hyper-Parameter Sweep on AlphaZero General.pdf:application/pdf},
}

@incollection{couetouxContinuousUpperConfidence2011,
	address = {Berlin, Heidelberg},
	title = {Continuous {Upper} {Confidence} {Trees}},
	volume = {6683},
	isbn = {978-3-642-25565-6 978-3-642-25566-3},
	url = {http://link.springer.com/10.1007/978-3-642-25566-3_32},
	abstract = {Upper Conﬁdence Trees are a very eﬃcient tool for solving Markov Decision Processes; originating in diﬃcult games like the game of Go, it is in particular surprisingly eﬃcient in high dimensional problems. It is known that it can be adapted to continuous domains in some cases (in particular continuous action spaces). We here present an extension of Upper Conﬁdence Trees to continuous stochastic problems. We (i) show a deceptive problem on which the classical Upper Conﬁdence Tree approach does not work, even with arbitrarily large computational power and with progressive widening (ii) propose an improvement, termed double-progressive widening, which takes care of the compromise between variance (we want inﬁnitely many simulations for each action/state) and bias (we want suﬃciently many nodes to avoid a bias by the ﬁrst nodes) and which extends the classical progressive widening (iii) discuss its consistency and show experimentally that it performs well on the deceptive problem and on experimental benchmarks. We guess that the double-progressive widening trick can be used for other algorithms as well, as a general tool for ensuring a good bias/variance compromise in search algorithms.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {Learning and {Intelligent} {Optimization}},
	publisher = {Springer Berlin Heidelberg},
	author = {Couëtoux, Adrien and Hoock, Jean-Baptiste and Sokolovska, Nataliya and Teytaud, Olivier and Bonnard, Nicolas},
	editor = {Coello, Carlos A. Coello},
	year = {2011},
	doi = {10.1007/978-3-642-25566-3_32},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {433--445},
	file = {Couëtoux et al. - 2011 - Continuous Upper Confidence Trees.pdf:/home/timo/Zotero/storage/356GHIRC/Couëtoux et al. - 2011 - Continuous Upper Confidence Trees.pdf:application/pdf},
}

@article{moerlandA0CAlphaZero2018,
	title = {{A0C}: {Alpha} {Zero} in {Continuous} {Action} {Space}},
	shorttitle = {{A0C}},
	url = {http://arxiv.org/abs/1805.09613},
	abstract = {A core novelty of Alpha Zero is the interleaving of tree search and deep learning, which has proven very successful in board games like Chess, Shogi and Go. These games have a discrete action space. However, many real-world reinforcement learning domains have continuous action spaces, for example in robotic control, navigation and self-driving cars. This paper presents the necessary theoretical extensions of Alpha Zero to deal with continuous action space. We also provide some preliminary experiments on the Pendulum swing-up task, empirically showing the feasibility of our approach. Thereby, this work provides a ﬁrst step towards the application of iterated search and learning in domains with a continuous action space.},
	language = {en},
	urldate = {2020-06-23},
	journal = {arXiv:1805.09613 [cs, stat]},
	author = {Moerland, Thomas M. and Broekens, Joost and Plaat, Aske and Jonker, Catholijn M.},
	month = may,
	year = {2018},
	note = {arXiv: 1805.09613},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Statistics - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
	file = {Moerland et al. - 2018 - A0C Alpha Zero in Continuous Action Space.pdf:/home/timo/Zotero/storage/X7GU5TAL/Moerland et al. - 2018 - A0C Alpha Zero in Continuous Action Space.pdf:application/pdf},
}

@article{lanMultiplePolicyValue,
	title = {Multiple {Policy} {Value} {Monte} {Carlo} {Tree} {Search}},
	abstract = {Many of the strongest game playing programs use a combination of Monte Carlo tree search (MCTS) and deep neural networks (DNN), where the DNNs are used as policy or value evaluators. Given a limited budget, such as online playing or during the self-play phase of AlphaZero (AZ) training, a balance needs to be reached between accurate state estimation and more MCTS simulations, both of which are critical for a strong game playing agent. Typically, larger DNNs are better at generalization and accurate evaluation, while smaller DNNs are less costly, and therefore can lead to more MCTS simulations and bigger search trees with the same budget. This paper introduces a new method called the multiple policy value MCTS (MPV-MCTS), which combines multiple policy value neural networks (PV-NNs) of various sizes to retain advantages of each network, where two PV-NNs fS and fL are used in this paper. We show through experiments on the game NoGo that a combined fS and fL MPV-MCTS outperforms single PV-NN with policy value MCTS, called PV-MCTS. Additionally, MPV-MCTS also outperforms PV-MCTS for AZ training.},
	language = {en},
	author = {Lan, Li-Cheng and Li, Wei and Wei, Ting-Han and Wu, I-Chen},
	pages = {7},
	file = {Lan et al. - Multiple Policy Value Monte Carlo Tree Search.pdf:/home/timo/Zotero/storage/ETE22QY5/Lan et al. - Multiple Policy Value Monte Carlo Tree Search.pdf:application/pdf},
}

@article{silverMasteringGameGo2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	language = {en},
	number = {7676},
	urldate = {2020-04-28},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	pages = {354--359},
	file = {Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:/home/timo/Zotero/storage/BATDY58Y/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf},
}

@article{vinyalsGrandmasterLevelStarCraft2019,
	title = {Grandmaster level in {StarCraft} {II} using multi-agent reinforcement learning},
	volume = {575},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-019-1724-z},
	doi = {10.1038/s41586-019-1724-z},
	language = {en},
	number = {7782},
	urldate = {2020-04-23},
	journal = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	month = nov,
	year = {2019},
	pages = {350--354},
	file = {Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agen.pdf:/home/timo/Zotero/storage/ZRGA2BKQ/Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agen.pdf:application/pdf},
}

@article{cobbeQuantifyingGeneralizationReinforcement,
	title = {Quantifying {Generalization} in {Reinforcement} {Learning}},
	abstract = {In this paper, we investigate the problem of overﬁtting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent’s ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we ﬁnd that agents overﬁt to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.},
	language = {en},
	author = {Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
	pages = {8},
	file = {Cobbe et al. - Quantifying Generalization in Reinforcement Learni.pdf:/home/timo/Zotero/storage/3M427NAS/Cobbe et al. - Quantifying Generalization in Reinforcement Learni.pdf:application/pdf},
}

@incollection{chaslotParallelMonteCarloTree2008,
	address = {Berlin, Heidelberg},
	title = {Parallel {Monte}-{Carlo} {Tree} {Search}},
	volume = {5131},
	isbn = {978-3-540-87607-6 978-3-540-87608-3},
	url = {http://link.springer.com/10.1007/978-3-540-87608-3_6},
	abstract = {Monte-Carlo Tree Search (MCTS) is a new best-ﬁrst search method that started a revolution in the ﬁeld of Computer Go. Parallelizing MCTS is an important way to increase the strength of any Go program. In this article, we discuss three parallelization methods for MCTS: leaf parallelization, root parallelization, and tree parallelization. To be eﬀective tree parallelization requires two techniques: adequately handling of (1) local mutexes and (2) virtual loss. Experiments in 13 × 13 Go reveal that in the program Mango root parallelization may lead to the best results for a speciﬁc time setting and speciﬁc program parameters. However, as soon as the selection mechanism is able to handle more adequately the balance of exploitation and exploration, tree parallelization should have attention too and could become a second choice for parallelizing MCTS. Preliminary experiments on the smaller 9 × 9 board provide promising prospects for tree parallelization.},
	language = {en},
	urldate = {2019-09-11},
	booktitle = {Computers and {Games}},
	publisher = {Springer Berlin Heidelberg},
	author = {Chaslot, Guillaume M. J. -B. and Winands, Mark H. M. and van den Herik, H. Jaap},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and van den Herik, H. Jaap and Xu, Xinhe and Ma, Zongmin and Winands, Mark H. M.},
	year = {2008},
	doi = {10.1007/978-3-540-87608-3_6},
	pages = {60--71},
	file = {Chaslot et al. - 2008 - Parallel Monte-Carlo Tree Search.pdf:/home/timo/Zotero/storage/QI836J7G/Chaslot et al. - 2008 - Parallel Monte-Carlo Tree Search.pdf:application/pdf},
}

@article{browneSurveyMonteCarlo2012,
	title = {A {Survey} of {Monte} {Carlo} {Tree} {Search} {Methods}},
	volume = {4},
	doi = {10.1109/TCIAIG.2012.2186810},
	abstract = {Monte Carlo tree search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarize the results from the key game and nongame domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.},
	number = {1},
	journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	author = {Browne, C. B. and Powley, E. and Whitehouse, D. and Lucas, S. M. and Cowling, P. I. and Rohlfshagen, P. and Tavener, S. and Perez, D. and Samothrakis, S. and Colton, S.},
	month = mar,
	year = {2012},
	keywords = {Artificial intelligence, Artificial intelligence (AI), bandit-based methods, computer Go, Computers, Decision theory, game search, game theory, Game theory, Games, key game, Markov processes, MCTS research, Monte Carlo methods, Monte Carlo tree search (MCTS), Monte carlo tree search methods, nongame domains, random sampling generality, tree searching, upper confidence bounds (UCB), upper confidence bounds for trees (UCT)},
	pages = {1--43},
	file = {IEEE Xplore Abstract Record:/home/timo/Zotero/storage/ME5UIW8N/6145622.html:text/html;Browne et al_2012_A Survey of Monte Carlo Tree Search Methods.pdf:/home/timo/Zotero/storage/9W8HRS2P/Browne et al_2012_A Survey of Monte Carlo Tree Search Methods.pdf:application/pdf},
}

@article{silverMasteringGameGo2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	language = {en},
	number = {7587},
	urldate = {2019-09-11},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	pages = {484--489},
	file = {Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:/home/timo/Zotero/storage/GRFAMNR6/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:application/pdf},
}

@article{haarnojaSoftActorCriticAlgorithms2018,
	title = {Soft {Actor}-{Critic} {Algorithms} and {Applications}},
	url = {http://arxiv.org/abs/1812.05905},
	abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy; that is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modiﬁcations that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as challenging real-world tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efﬁciency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
	language = {en},
	urldate = {2019-05-16},
	journal = {arXiv:1812.05905 [cs, stat]},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.05905},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Statistics - Machine Learning},
	file = {Haarnoja et al. - 2018 - Soft Actor-Critic Algorithms and Applications.pdf:/home/timo/Zotero/storage/ILB3HJQ2/Haarnoja et al. - 2018 - Soft Actor-Critic Algorithms and Applications.pdf:application/pdf},
}

@article{schrittwieserMasteringAtariGo2020,
	title = {Mastering {Atari}, {Go}, {Chess} and {Shogi} by {Planning} with a {Learned} {Model}},
	url = {http://arxiv.org/abs/1911.08265},
	abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artiﬁcial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
	language = {en},
	urldate = {2020-10-25},
	journal = {arXiv:1911.08265 [cs, stat]},
	author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
	month = feb,
	year = {2020},
	note = {arXiv: 1911.08265},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Schrittwieser et al. - 2020 - Mastering Atari, Go, Chess and Shogi by Planning w.pdf:/home/timo/Zotero/storage/IBNAF2WE/Schrittwieser et al. - 2020 - Mastering Atari, Go, Chess and Shogi by Planning w.pdf:application/pdf},
}

@article{wangPolicyValueLoss,
	title = {Policy or {Value}? {Loss} {Function} and {Playing} {Strength} in {AlphaZero}-like {Self}-play},
	abstract = {Recently, AlphaZero has achieved outstanding performance in playing Go, Chess, and Shogi. Players in AlphaZero consist of a combination of Monte Carlo Tree Search and a Deep Q-network, that is trained using self-play. The uniﬁed Deep Q-network has a policy-head and a value-head. In AlphaZero, during training, the optimization minimizes the sum of the policy loss and the value loss. However, it is not clear if and under which circumstances other formulations of the objective function are better. Therefore, in this paper, we perform experiments with combinations of these two optimization targets. Self-play is a computationally intensive method. By using small games, we are able to perform multiple test cases. We use a light-weight open source reimplementation of AlphaZero on two different games. We investigate optimizing the two targets independently, and also try different combinations (sum and product).},
	language = {en},
	author = {Wang, Hui and Emmerich, Michael and Preuss, Mike and Plaat, Aske},
	pages = {8},
	file = {Wang et al. - Policy or Value Loss Function and Playing Strengt.pdf:/home/timo/Zotero/storage/N53IEM5Z/Wang et al. - Policy or Value Loss Function and Playing Strengt.pdf:application/pdf},
}

@article{schulmanProximalPolicyOptimization2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the beneﬁts of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	language = {en},
	urldate = {2020-10-16},
	journal = {arXiv:1707.06347 [cs]},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv: 1707.06347},
	keywords = {Computer Science - Machine Learning},
	file = {Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:/home/timo/Zotero/storage/NBJM8DDA/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf},
}

@article{baLayerNormalization2016,
	title = {Layer {Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This signiﬁcantly reduces the training time in feedforward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	language = {en},
	urldate = {2020-09-20},
	journal = {arXiv:1607.06450 [cs, stat]},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.06450},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Ba et al. - 2016 - Layer Normalization.pdf:/home/timo/Zotero/storage/VP8VXCNZ/Ba et al. - 2016 - Layer Normalization.pdf:application/pdf},
}

@book{suttonReinforcementLearningIntroduction2018,
	address = {Cambridge, Massachusetts},
	edition = {Second edition},
	series = {Adaptive computation and machine learning series},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
	keywords = {Reinforcement learning},
	file = {Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:/home/timo/Zotero/storage/78X6Y49M/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:application/pdf},
}

@incollection{guptaCooperativeMultiagentControl2017,
	address = {Cham},
	title = {Cooperative {Multi}-agent {Control} {Using} {Deep} {Reinforcement} {Learning}},
	volume = {10642},
	isbn = {978-3-319-71681-7 978-3-319-71682-4},
	url = {http://link.springer.com/10.1007/978-3-319-71682-4_5},
	abstract = {This work considers the problem of learning cooperative policies in complex, partially observable domains without explicit communication. We extend three classes of single-agent deep reinforcement learning algorithms based on policy gradient, temporal-difference error, and actor-critic methods to cooperative multi-agent systems. We introduce a set of cooperative control tasks that includes tasks with discrete and continuous actions, as well as tasks that involve hundreds of agents. The three approaches are evaluated against each other using different neural architectures, training procedures, and reward structures. Using deep reinforcement learning with a curriculum learning scheme, our approach can solve problems that were previously considered intractable by most multi-agent reinforcement learning algorithms. We show that policy gradient methods tend to outperform both temporal-difference and actor-critic methods when using feed-forward neural architectures. We also show that recurrent policies, while more difﬁcult to train, outperform feed-forward policies on our evaluation tasks.},
	language = {en},
	urldate = {2020-09-17},
	booktitle = {Autonomous {Agents} and {Multiagent} {Systems}},
	publisher = {Springer International Publishing},
	author = {Gupta, Jayesh K. and Egorov, Maxim and Kochenderfer, Mykel},
	editor = {Sukthankar, Gita and Rodriguez-Aguilar, Juan A.},
	year = {2017},
	doi = {10.1007/978-3-319-71682-4_5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {66--83},
	file = {Gupta et al. - 2017 - Cooperative Multi-agent Control Using Deep Reinfor.pdf:/home/timo/Zotero/storage/9BSM7PPN/Gupta et al. - 2017 - Cooperative Multi-agent Control Using Deep Reinfor.pdf:application/pdf},
}

@article{everettCollisionAvoidancePedestrianRich2020,
	title = {Collision {Avoidance} in {Pedestrian}-{Rich} {Environments} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1910.11689},
	abstract = {Collision avoidance algorithms are essential for safe and efﬁcient robot operation among pedestrians. This work proposes using deep reinforcement (RL) learning as a framework to model the complex interactions and cooperation with nearby, decision-making agents, such as pedestrians and other robots. Existing RL-based works assume homogeneity of agent properties, use speciﬁc motion models over short timescales, or lack a principled method to handle a large, possibly varying number of agents. Therefore, this work develops an algorithm that learns collision avoidance among a variety of heterogeneous, non-communicating, dynamic agents without assuming they follow any particular behavior rules. It extends our previous work by introducing a strategy using Long Short-Term Memory (LSTM) that enables the algorithm to use observations of an arbitrary number of other agents, instead of a small, ﬁxed number of neighbors. The proposed algorithm is shown to outperform a classical collision avoidance algorithm, another deep RL-based algorithm, and scales with the number of agents better (fewer collisions, shorter time to goal) than our previously published learning-based approach. Analysis of the LSTM provides insights into how observations of nearby agents affect the hidden state and quantiﬁes the performance impact of various agent ordering heuristics. The learned policy generalizes to several applications beyond the training scenarios: formation control (arrangement into letters), demonstrations on a ﬂeet of four multirotors and on a fully autonomous robotic vehicle capable of traveling at human walking speed among pedestrians.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1910.11689 [cs]},
	author = {Everett, Michael and Chen, Yu Fan and How, Jonathan P.},
	month = apr,
	year = {2020},
	note = {arXiv: 1910.11689},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {Everett et al. - 2020 - Collision Avoidance in Pedestrian-Rich Environment.pdf:/home/timo/Zotero/storage/PDIBFMZP/Everett et al. - 2020 - Collision Avoidance in Pedestrian-Rich Environment.pdf:application/pdf},
}

@article{hoelCombiningPlanningDeep2020,
	title = {Combining {Planning} and {Deep} {Reinforcement} {Learning} in {Tactical} {Decision} {Making} for {Autonomous} {Driving}},
	volume = {5},
	issn = {2379-8904, 2379-8858},
	url = {http://arxiv.org/abs/1905.02680},
	doi = {10.1109/TIV.2019.2955905},
	abstract = {Tactical decision making for autonomous driving is challenging due to the diversity of environments, the uncertainty in the sensor information, and the complex interaction with other road users. This paper introduces a general framework for tactical decision making, which combines the concepts of planning and learning, in the form of Monte Carlo tree search and deep reinforcement learning. The method is based on the AlphaGo Zero algorithm, which is extended to a domain with a continuous state space where self-play cannot be used. The framework is applied to two different highway driving cases in a simulated environment and it is shown to perform better than a commonly used baseline method. The strength of combining planning and learning is also illustrated by a comparison to using the Monte Carlo tree search or the neural network policy separately.},
	language = {en},
	number = {2},
	urldate = {2020-09-17},
	journal = {IEEE Transactions on Intelligent Vehicles},
	author = {Hoel, Carl-Johan and Driggs-Campbell, Katherine and Wolff, Krister and Laine, Leo and Kochenderfer, Mykel J.},
	month = jun,
	year = {2020},
	note = {arXiv: 1905.02680},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	pages = {294--305},
	file = {Hoel et al. - 2020 - Combining Planning and Deep Reinforcement Learning.pdf:/home/timo/Zotero/storage/VNNFJFS9/Hoel et al. - 2020 - Combining Planning and Deep Reinforcement Learning.pdf:application/pdf},
}

@article{moritzRayDistributedFramework2018,
	title = {Ray: {A} {Distributed} {Framework} for {Emerging} {AI} {Applications}},
	shorttitle = {Ray},
	url = {http://arxiv.org/abs/1712.05889},
	abstract = {The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and ﬂexibility. In this paper, we consider these requirements and present Ray—a distributed system to address them. Ray implements a uniﬁed interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system’s control state. In our experiments, we demonstrate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1712.05889 [cs, stat]},
	author = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I. and Stoica, Ion},
	month = sep,
	year = {2018},
	note = {arXiv: 1712.05889},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Moritz et al. - 2018 - Ray A Distributed Framework for Emerging AI Appli.pdf:/home/timo/Zotero/storage/PXLQE2K5/Moritz et al. - 2018 - Ray A Distributed Framework for Emerging AI Appli.pdf:application/pdf},
}

@article{paszkePyTorchImperativeStyle,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientiﬁc computing libraries, while remaining efﬁcient and supporting hardware accelerators such as GPUs.},
	language = {en},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	pages = {12},
	file = {Paszke et al. - PyTorch An Imperative Style, High-Performance Dee.pdf:/home/timo/Zotero/storage/YTJ7CSYJ/Paszke et al. - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf},
}

@article{quigleyROSOpensourceRobot,
	title = {{ROS}: an open-source {Robot} {Operating} {System}},
	abstract = {This paper gives an overview of ROS, an opensource robot operating system. ROS is not an operating system in the traditional sense of process management and scheduling; rather, it provides a structured communications layer above the host operating systems of a heterogenous compute cluster. In this paper, we discuss how ROS relates to existing robot software frameworks, and brieﬂy overview some of the available application software which uses ROS.},
	language = {en},
	author = {Quigley, Morgan and Gerkey, Brian and Conley, Ken and Faust, Josh and Foote, Tully and Leibs, Jeremy and Berger, Eric and Wheeler, Rob and Ng, Andrew},
	pages = {6},
	file = {Quigley et al. - ROS an open-source Robot Operating System.pdf:/home/timo/Zotero/storage/ZUPVVN6G/Quigley et al. - ROS an open-source Robot Operating System.pdf:application/pdf},
}

@article{giuliariTransformerNetworksTrajectory2020,
	title = {Transformer {Networks} for {Trajectory} {Forecasting}},
	url = {http://arxiv.org/abs/2003.08111},
	abstract = {Most recent successes on forecasting the people motion are based on LSTM models and all most recent progress has been achieved by modelling the social interaction among people and the people interaction with the scene. We question the use of the LSTM models and propose the novel use of Transformer Networks for trajectory forecasting. This is a fundamental switch from the sequential step-by-step processing of LSTMs to the only-attention-based memory mechanisms of Transformers. In particular, we consider both the original Transformer Network (TF) and the larger Bidirectional Transformer (BERT), state-of-the-art on all natural language processing tasks. Our proposed Transformers predict the trajectories of the individual people in the scene. These are "simple" model because each person is modelled separately without any complex human-human nor scene interaction terms. In particular, the TF model without bells and whistles yields the best score on the largest and most challenging trajectory forecasting benchmark of TrajNet. Additionally, its extension which predicts multiple plausible future trajectories performs on par with more engineered techniques on the 5 datasets of ETH + UCY. Finally, we show that Transformers may deal with missing observations, as it may be the case with real sensor data. Code is available at https://github.com/FGiuliari/Trajectory-Transformer.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:2003.08111 [cs]},
	author = {Giuliari, Francesco and Hasan, Irtiza and Cristani, Marco and Galasso, Fabio},
	month = may,
	year = {2020},
	note = {arXiv: 2003.08111},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Giuliari et al. - 2020 - Transformer Networks for Trajectory Forecasting.pdf:/home/timo/Zotero/storage/S2W299C9/Giuliari et al. - 2020 - Transformer Networks for Trajectory Forecasting.pdf:application/pdf},
}

@article{kimMultiHeadAttentionBased2020,
	title = {Multi-{Head} {Attention} based {Probabilistic} {Vehicle} {Trajectory} {Prediction}},
	url = {http://arxiv.org/abs/2004.03842},
	abstract = {This paper presents online-capable deep learning model for probabilistic vehicle trajectory prediction. We propose a simple encoder-decoder architecture based on multihead attention. The proposed model generates the distribution of the predicted trajectories for multiple vehicles in parallel. Our approach to model the interactions can learn to attend to a few inﬂuential vehicles in an unsupervised manner, which can improve the interpretability of the network. The experiments using naturalistic trajectories at highway show the clear improvement in terms of positional error on both longitudinal and lateral direction.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:2004.03842 [cs, eess]},
	author = {Kim, Hayoung and Kim, Dongchan and Kim, Gihoon and Cho, Jeongmin and Huh, Kunsoo},
	month = jul,
	year = {2020},
	note = {arXiv: 2004.03842},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Electrical Engineering and Systems Science - Signal Processing},
	file = {Kim et al. - 2020 - Multi-Head Attention based Probabilistic Vehicle T.pdf:/home/timo/Zotero/storage/P98ACB9X/Kim et al. - 2020 - Multi-Head Attention based Probabilistic Vehicle T.pdf:application/pdf},
}

@article{harrisArrayProgrammingNumPy2020,
	title = {Array programming with {NumPy}},
	volume = {585},
	copyright = {2020 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2649-2},
	doi = {10.1038/s41586-020-2649-2},
	abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
	language = {en},
	number = {7825},
	urldate = {2020-09-17},
	journal = {Nature},
	author = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, Stéfan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and del Río, Jaime Fernández and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	month = sep,
	year = {2020},
	note = {Number: 7825
Publisher: Nature Publishing Group},
	pages = {357--362},
	file = {Full Text PDF:/home/timo/Zotero/storage/MZXPY36B/Harris et al. - 2020 - Array programming with NumPy.pdf:application/pdf},
}

@inproceedings{huberEntropyApproximationGaussian2008,
	address = {Seoul},
	title = {On entropy approximation for {Gaussian} mixture random vectors},
	isbn = {978-1-4244-2143-5},
	url = {http://ieeexplore.ieee.org/document/4648062/},
	doi = {10.1109/MFI.2008.4648062},
	abstract = {For many practical probability density representations such as for the widely used Gaussian mixture densities, an analytic evaluation of the differential entropy is not possible and thus, approximate calculations are inevitable. For this purpose, the ﬁrst contribution of this paper deals with a novel entropy approximation method for Gaussian mixture random vectors, which is based on a component-wise Taylor-series expansion of the logarithm of a Gaussian mixture and on a splitting method of Gaussian mixture components. The employed order of the Taylor-series expansion and the number of components used for splitting allows balancing between accuracy and computational demand. The second contribution is the determination of meaningful and efﬁciently to calculate lower and upper bounds of the entropy, which can be also used for approximation purposes. In addition, a reﬁnement method for the more important upper bound is proposed in order to approach the true entropy value.},
	language = {en},
	urldate = {2020-09-15},
	booktitle = {2008 {IEEE} {International} {Conference} on {Multisensor} {Fusion} and {Integration} for {Intelligent} {Systems}},
	publisher = {IEEE},
	author = {Huber, Marco F. and Bailey, Tim and Durrant-Whyte, Hugh and Hanebeck, Uwe D.},
	month = aug,
	year = {2008},
	pages = {181--188},
	file = {Huber et al. - 2008 - On entropy approximation for Gaussian mixture rand.pdf:/home/timo/Zotero/storage/G5HBF5NT/Huber et al. - 2008 - On entropy approximation for Gaussian mixture rand.pdf:application/pdf},
}

@inproceedings{takMonteCarloTree2014,
	address = {Dortmund, Germany},
	title = {Monte {Carlo} {Tree} {Search} variants for simultaneous move games},
	isbn = {978-1-4799-3547-5},
	url = {http://ieeexplore.ieee.org/document/6932889/},
	doi = {10.1109/CIG.2014.6932889},
	abstract = {Monte Carlo Tree Search (MCTS) is a widely-used technique for game-tree search in sequential turn-based games. The extension to simultaneous move games, where all players choose moves simultaneously each turn, is non-trivial due to the complexity of this class of games. In this paper, we describe simultaneous move MCTS and analyze its application in a set of nine disparate simultaneous move games. We use several possible variants, Decoupled UCT, Sequential UCT, Exp3, and Regret Matching. These variants include both deterministic and stochastic selection strategies and we characterize the game-play performance of each one. The results indicate that the relative performance of each variant depends strongly on the game and the opponent, and that parameter tuning can also not be as straightforward as the purely sequential case. Overall, Decoupled UCT performs best despite its theoretical shortcomings.},
	language = {en},
	urldate = {2020-09-14},
	booktitle = {2014 {IEEE} {Conference} on {Computational} {Intelligence} and {Games}},
	publisher = {IEEE},
	author = {Tak, Mandy J. W. and Lanctot, Marc and Winands, Mark H. M.},
	month = aug,
	year = {2014},
	pages = {1--8},
	file = {Tak et al. - 2014 - Monte Carlo Tree Search variants for simultaneous .pdf:/home/timo/Zotero/storage/Z4CBGY7B/Tak et al. - 2014 - Monte Carlo Tree Search variants for simultaneous .pdf:application/pdf},
}

@article{lanctotMonteCarloTree,
	title = {Monte {Carlo} {Tree} {Search} for {Simultaneous} {Move} {Games}: {A} {Case} {Study} in the {Game} of {Tron}},
	abstract = {MCTS has been successfully applied to many sequential games. This paper investigates Monte Carlo Tree Search (MCTS) for the simultaneous move game Tron. In this paper we describe two different ways to model the simultaneous move game, as a standard sequential game and as a stacked matrix game. Several variants are presented to adapt MCTS to simultaneous move games, such as Sequential UCT, Decoupled UCT, Exp3, and a novel stochastic method based on Regret Matching. Through the experiments in the game of Tron on four different boards, it is shown that Decoupled UCB1-Tuned perform best, winning 62.3\% of games overall. We also show that Regret Matching wins 53.1\% of games overall and search techniques that model the game sequentially win 51.4-54.3\% of games overall.},
	language = {en},
	author = {Lanctot, Marc and Wittlinger, Christopher and Winands, Mark H M},
	pages = {8},
	file = {Lanctot et al. - Monte Carlo Tree Search for Simultaneous Move Game.pdf:/home/timo/Zotero/storage/Z5AZK9UR/Lanctot et al. - Monte Carlo Tree Search for Simultaneous Move Game.pdf:application/pdf},
}

@article{devlinBERTPretrainingDeep2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2020-09-10},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:/home/timo/Zotero/storage/JHZKAJJ7/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@article{haarnojaSoftActorCriticOffPolicy2018,
	title = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
	shorttitle = {Soft {Actor}-{Critic}},
	url = {http://arxiv.org/abs/1801.01290},
	abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
	language = {en},
	urldate = {2020-08-31},
	journal = {arXiv:1801.01290 [cs, stat]},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	month = aug,
	year = {2018},
	note = {arXiv: 1801.01290},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf:/home/timo/Zotero/storage/FUXWDWX2/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf:application/pdf},
}

@article{lillicrapContinuousControlDeep2019,
	title = {Continuous control with deep reinforcement learning},
	url = {http://arxiv.org/abs/1509.02971},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to ﬁnd policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies “end-to-end”: directly from raw pixel inputs.},
	language = {en},
	urldate = {2020-08-19},
	journal = {arXiv:1509.02971 [cs, stat]},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	month = jul,
	year = {2019},
	note = {arXiv: 1509.02971},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Lillicrap et al. - 2019 - Continuous control with deep reinforcement learnin.pdf:/home/timo/Zotero/storage/LTJALUXX/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learnin.pdf:application/pdf},
}

@article{mnihHumanlevelControlDeep2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	language = {en},
	number = {7540},
	urldate = {2020-08-19},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	pages = {529--533},
	file = {Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:/home/timo/Zotero/storage/CI67NUV6/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf},
}

@article{hugleDynamicInputDeep2019,
	title = {Dynamic {Input} for {Deep} {Reinforcement} {Learning} in {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/1907.10994},
	doi = {10.1109/IROS40897.2019.8968560},
	abstract = {In many real-world decision making problems, reaching an optimal decision requires taking into account a variable number of objects around the agent. Autonomous driving is a domain in which this is especially relevant, since the number of cars surrounding the agent varies considerably over time and affects the optimal action to be taken. Classical methods that process object lists can deal with this requirement. However, to take advantage of recent high-performing methods based on deep reinforcement learning in modular pipelines, special architectures are necessary. For these, a number of options exist, but a thorough comparison of the different possibilities is missing. In this paper, we elaborate limitations of fullyconnected neural networks and other established approaches like convolutional and recurrent neural networks in the context of reinforcement learning problems that have to deal with variable sized inputs. We employ the structure of Deep Sets in off-policy reinforcement learning for high-level decision making, highlight their capabilities to alleviate these limitations, and show that Deep Sets not only yield the best overall performance but also offer better generalization to unseen situations than the other approaches.},
	language = {en},
	urldate = {2021-02-07},
	journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	author = {Hügle, Maria and Kalweit, Gabriel and Mirchevska, Branka and Werling, Moritz and Boedecker, Joschka},
	month = nov,
	year = {2019},
	note = {arXiv: 1907.10994},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
	pages = {7566--7573},
	file = {Hügle et al. - 2019 - Dynamic Input for Deep Reinforcement Learning in A.pdf:/home/timo/Zotero/storage/8FXGQ9GJ/Hügle et al. - 2019 - Dynamic Input for Deep Reinforcement Learning in A.pdf:application/pdf},
}

@article{huegleDynamicInteractionAwareScene2019,
	title = {Dynamic {Interaction}-{Aware} {Scene} {Understanding} for {Reinforcement} {Learning} in {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/1909.13582},
	abstract = {The common pipeline in autonomous driving systems is highly modular and includes a perception component which extracts lists of surrounding objects and passes these lists to a high-level decision component. In this case, leveraging the beneﬁts of deep reinforcement learning for high-level decision making requires special architectures to deal with multiple variable-length sequences of different object types, such as vehicles, lanes or trafﬁc signs. At the same time, the architecture has to be able to cover interactions between trafﬁc participants in order to ﬁnd the optimal action to be taken. In this work, we propose the novel Deep Scenes architecture, that can learn complex interaction-aware scene representations based on extensions of either 1) Deep Sets or 2) Graph Convolutional Networks. We present the Graph-Q and DeepScene-Q off-policy reinforcement learning algorithms, both outperforming state-ofthe-art methods in evaluations with the publicly available trafﬁc simulator SUMO.},
	language = {en},
	urldate = {2021-02-07},
	journal = {arXiv:1909.13582 [cs, stat]},
	author = {Huegle, Maria and Kalweit, Gabriel and Werling, Moritz and Boedecker, Joschka},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.13582},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Statistics - Machine Learning},
	file = {Huegle et al. - 2019 - Dynamic Interaction-Aware Scene Understanding for .pdf:/home/timo/Zotero/storage/C5JN233Z/Huegle et al. - 2019 - Dynamic Interaction-Aware Scene Understanding for .pdf:application/pdf},
}

@incollection{couetouxImprovingExplorationUpper2012,
	address = {Berlin, Heidelberg},
	title = {Improving the {Exploration} in {Upper} {Confidence} {Trees}},
	volume = {7219},
	isbn = {978-3-642-34412-1 978-3-642-34413-8},
	url = {http://link.springer.com/10.1007/978-3-642-34413-8_29},
	abstract = {In the standard version of the UCT algorithm, in the case of a continuous set of decisions, the exploration of new decisions is done through blind search. This can lead to very ineﬃcient exploration, particularly in the case of large dimension problems, which often happens in energy management problems, for instance. In an attempt to use the information gathered through past simulations to better explore new decisions, we propose a method named Blind Value (BV). It only requires the access to a function that randomly draws feasible decisions. We also implement it and compare it to the original version of continuous UCT. Our results show that it gives a signiﬁcant increase in convergence speed, in dimensions 12 and 80.},
	language = {en},
	urldate = {2021-02-19},
	booktitle = {Learning and {Intelligent} {Optimization}},
	publisher = {Springer Berlin Heidelberg},
	author = {Couëtoux, Adrien and Doghmen, Hassen and Teytaud, Olivier},
	editor = {Hamadi, Youssef and Schoenauer, Marc},
	year = {2012},
	doi = {10.1007/978-3-642-34413-8_29},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {366--371},
	file = {Couëtoux et al. - 2012 - Improving the Exploration in Upper Confidence Tree.pdf:/home/timo/Zotero/storage/DKKWM7T4/Couëtoux et al. - 2012 - Improving the Exploration in Upper Confidence Tree.pdf:application/pdf},
}

@article{hubertLearningPlanningComplex2021,
	title = {Learning and {Planning} in {Complex} {Action} {Spaces}},
	url = {http://arxiv.org/abs/2104.06303},
	abstract = {Many important real-world problems have action spaces that are high-dimensional, continuous or both, making full enumeration of all possible actions infeasible. Instead, only small subsets of actions can be sampled for the purpose of policy evaluation and improvement. In this paper, we propose a general framework to reason in a principled way about policy evaluation and improvement over such sampled action subsets. This sample-based policy iteration framework can in principle be applied to any reinforcement learning algorithm based upon policy iteration. Concretely, we propose Sampled MuZero, an extension of the MuZero algorithm that is able to learn in domains with arbitrarily complex action spaces by planning over sampled actions. We demonstrate this approach on the classical board game of Go and on two continuous control benchmark domains: DeepMind Control Suite and Real-World RL Suite.},
	language = {en},
	urldate = {2021-04-14},
	journal = {arXiv:2104.06303 [cs]},
	author = {Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Barekatain, Mohammadamin and Schmitt, Simon and Silver, David},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.06303},
	keywords = {Computer Science - Machine Learning},
	file = {Hubert et al. - 2021 - Learning and Planning in Complex Action Spaces.pdf:/home/timo/Zotero/storage/WLG9K54D/Hubert et al. - 2021 - Learning and Planning in Complex Action Spaces.pdf:application/pdf},
}

@article{dulac-arnoldEmpiricalInvestigationChallenges2021,
	title = {An empirical investigation of the challenges of real-world reinforcement learning},
	url = {http://arxiv.org/abs/2003.11881},
	abstract = {Reinforcement learning (RL) has proven its worth in a series of artiﬁcial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are hard to leverage in real-world systems due to a series of assumptions that are rarely satisﬁed in practice. In this work, we identify and formalize a series of independent challenges that embody the difﬁculties that must be addressed for RL to be commonly deployed in real-world systems. For each challenge, we deﬁne it formally in the context of a Markov Decision Process, analyze the effects of the challenge on state-of-the-art learning algorithms, and present some existing attempts at tackling it. We believe that an approach that addresses our set of proposed challenges would be readily deployable in a large number of real world problems. Our proposed challenges are implemented in a suite of continuous control environments called realworldrl-suite which we propose an as an open-source benchmark.},
	language = {en},
	urldate = {2021-05-20},
	journal = {arXiv:2003.11881 [cs]},
	author = {Dulac-Arnold, Gabriel and Levine, Nir and Mankowitz, Daniel J. and Li, Jerry and Paduraru, Cosmin and Gowal, Sven and Hester, Todd},
	month = mar,
	year = {2021},
	note = {arXiv: 2003.11881},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Dulac-Arnold et al. - 2021 - An empirical investigation of the challenges of re.pdf:/home/timo/Zotero/storage/Q3MPRM9Z/Dulac-Arnold et al. - 2021 - An empirical investigation of the challenges of re.pdf:application/pdf},
}

@article{chouImprovingStochasticPolicy,
	title = {Improving {Stochastic} {Policy} {Gradients} in {Continuous} {Control} with {Deep} {Reinforcement} {Learning} using the {Beta} {Distribution}},
	abstract = {Recently, reinforcement learning with deep neural networks has achieved great success in challenging continuous control problems such as 3D locomotion and robotic manipulation. However, in real-world control problems, the actions one can take are bounded by physical constraints, which introduces a bias when the standard Gaussian distribution is used as the stochastic policy. In this work, we propose to use the Beta distribution as an alternative and analyze the bias and variance of the policy gradients of both policies. We show that the Beta policy is bias-free and provides signiﬁcantly faster convergence and higher scores over the Gaussian policy when both are used with trust region policy optimization (TRPO) and actor critic with experience replay (ACER), the state-of-the-art on- and offpolicy stochastic methods respectively, on OpenAI Gym’s and MuJoCo’s continuous control environments.},
	language = {en},
	author = {Chou, Po-Wei and Maturana, Daniel and Scherer, Sebastian},
	pages = {10},
	file = {Chou et al. - Improving Stochastic Policy Gradients in Continuou.pdf:/home/timo/Zotero/storage/RHXV2H97/Chou et al. - Improving Stochastic Policy Gradients in Continuou.pdf:application/pdf},
}

@article{brockmanOpenAIGym2016,
	title = {{OpenAI} {Gym}},
	url = {http://arxiv.org/abs/1606.01540},
	abstract = {OpenAI Gym1 is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1606.01540 [cs]},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.01540},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Brockman et al. - 2016 - OpenAI Gym.pdf:/home/timo/Zotero/storage/6HYW6JDL/Brockman et al. - 2016 - OpenAI Gym.pdf:application/pdf},
}

@article{williamsSimpleStatisticalGradientfollowing,
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	abstract = {This article presents a general class of associativereinforcementlearning algorithmsfor connectionist networkscontainingstochasticunits. These algorithms,calledREINFORCEalgorithms,are shownto makeweight adjustmentsin a direction that lies alongthe gradient of expectedreinforcementin both immediate-reinforcement tasks and certain limited forms of delayed-reinforcementtasks, and they do this without explicitly computing gradient estimatesor even storing informationfrom which such estimates couldbe computed. Specificexamples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novelbut potentiallyinterestingin their own right. Also givenare resultsthat showhow such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerationsthat might be used to help developsimilar but potentially more powerfulreinforcement learning algorithms.},
	language = {en},
	author = {Williams, Ronald J},
	pages = {28},
	file = {Williams - Simple statistical gradient-following algorithms f.pdf:/home/timo/Zotero/storage/66F7K6N9/Williams - Simple statistical gradient-following algorithms f.pdf:application/pdf},
}

@article{tayLongRangeArena2020,
	title = {Long {Range} {Arena}: {A} {Benchmark} for {Efficient} {Transformers}},
	shorttitle = {Long {Range} {Arena}},
	url = {http://arxiv.org/abs/2011.04006},
	abstract = {Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efﬁcient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difﬁcult to assess relative model quality amongst many models. This paper proposes a systematic and uniﬁed benchmark, Long-Range Arena, speciﬁcally focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. Long-Range Arena paves the way towards better understanding this class of efﬁcient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at https://github.com/google-research/long-range-arena.},
	language = {en},
	urldate = {2021-06-09},
	journal = {arXiv:2011.04006 [cs]},
	author = {Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.04006},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Tay et al. - 2020 - Long Range Arena A Benchmark for Efficient Transf.pdf:/home/timo/Zotero/storage/W7TJRA2M/Tay et al. - 2020 - Long Range Arena A Benchmark for Efficient Transf.pdf:application/pdf},
}

@article{foersterDeepMultiAgentReinforcement,
	title = {Deep {Multi}-{Agent} {Reinforcement} {Learning}},
	language = {en},
	author = {Foerster, Jakob N},
	pages = {205},
	file = {Foerster - Deep Multi-Agent Reinforcement Learning.pdf:/home/timo/Zotero/storage/VQJT3N9Y/Foerster - Deep Multi-Agent Reinforcement Learning.pdf:application/pdf},
}

@incollection{kocsisBanditBasedMonteCarlo2006,
	address = {Berlin, Heidelberg},
	title = {Bandit {Based} {Monte}-{Carlo} {Planning}},
	volume = {4212},
	isbn = {978-3-540-45375-8 978-3-540-46056-5},
	url = {http://link.springer.com/10.1007/11871842_29},
	abstract = {For large state-space Markovian Decision Problems MonteCarlo planning is one of the few viable approaches to ﬁnd near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In ﬁnite-horizon or discounted MDPs the algorithm is shown to be consistent and ﬁnite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is signiﬁcantly more eﬃcient than its alternatives.},
	language = {en},
	urldate = {2021-06-11},
	booktitle = {Machine {Learning}: {ECML} 2006},
	publisher = {Springer Berlin Heidelberg},
	author = {Kocsis, Levente and Szepesvári, Csaba},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Fürnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
	year = {2006},
	doi = {10.1007/11871842_29},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {282--293},
	file = {Kocsis and Szepesvári - 2006 - Bandit Based Monte-Carlo Planning.pdf:/home/timo/Zotero/storage/CZP5VM4J/Kocsis and Szepesvári - 2006 - Bandit Based Monte-Carlo Planning.pdf:application/pdf},
}

@article{kocsisImprovedMonteCarloSearch,
	title = {Improved {Monte}-{Carlo} {Search}},
	abstract = {Monte-Carlo search has been successful in many non-deterministic games, and recently in deterministic games with high branching factor. One of the drawbacks of the current approaches is that even if the iterative process would last for a very long time, the selected move does not necessarily converge to a game-theoretic optimal one. In this paper we introduce a new algorithm, UCT, which extends a bandit algorithm for Monte-Carlo search. It is proven that the probability that the algorithm selects the correct move converges to 1. Moreover it is shown empirically that the algorithm converges rather fast even in comparison with alpha-beta search. Experiments in Amazons and Clobber indicate that the UCT algorithm outperforms considerably a plain Monte-Carlo version, and it is competitive against alpha-beta based game programs.},
	language = {en},
	author = {Kocsis, Levente and Szepesvari, Csaba and Willemson, Jan},
	pages = {22},
	file = {Kocsis et al. - Improved Monte-Carlo Search.pdf:/home/timo/Zotero/storage/2W9CXMC5/Kocsis et al. - Improved Monte-Carlo Search.pdf:application/pdf},
}

@article{francois-lavetIntroductionDeepReinforcement2018,
	title = {An {Introduction} to {Deep} {Reinforcement} {Learning}},
	volume = {11},
	issn = {1935-8237, 1935-8245},
	url = {http://arxiv.org/abs/1811.12560},
	doi = {10.1561/2200000071},
	abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This ﬁeld of research has been able to solve a wide range of complex decisionmaking tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, ﬁnance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
	language = {en},
	number = {3-4},
	urldate = {2021-06-17},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Francois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
	year = {2018},
	note = {arXiv: 1811.12560},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	pages = {219--354},
	file = {Francois-Lavet et al. - 2018 - An Introduction to Deep Reinforcement Learning.pdf:/home/timo/Zotero/storage/ZVR7XUXQ/Francois-Lavet et al. - 2018 - An Introduction to Deep Reinforcement Learning.pdf:application/pdf},
}

@article{hongModelbasedLookaheadReinforcement2019,
	title = {Model-based {Lookahead} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1908.06012},
	abstract = {Model-based Reinforcement Learning (MBRL) allows data-efﬁcient learning which is required in real world applications such as robotics. However, despite the impressive data-efﬁciency, MBRL does not achieve the ﬁnal performance of state-ofthe-art Model-free Reinforcement Learning (MFRL) methods. We leverage the strengths of both realms and propose an approach that obtains high performance with a small amount of data. In particular, we combine MFRL and Model Predictive Control (MPC). While MFRL’s strength in exploration allows us to train a better forward dynamics model for MPC, MPC improves the performance of the MFRL policy by sampling-based planning. The experimental results in standard continuous control benchmarks show that our approach can achieve MFRL‘s level of performance while being as data-efﬁcient as MBRL.},
	language = {en},
	urldate = {2021-07-01},
	journal = {arXiv:1908.06012 [cs, stat]},
	author = {Hong, Zhang-Wei and Pajarinen, Joni and Peters, Jan},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.06012},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Hong et al. - 2019 - Model-based Lookahead Reinforcement Learning.pdf:/home/timo/Zotero/storage/CBU28HGI/Hong et al. - 2019 - Model-based Lookahead Reinforcement Learning.pdf:application/pdf},
}

@article{lecunBackpropagationAppliedHandwritten1989,
	title = {Backpropagation {Applied} to {Handwritten} {Zip} {Code} {Recognition}},
	volume = {1},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/1/4/541-551/5515},
	doi = {10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	language = {en},
	number = {4},
	urldate = {2021-07-08},
	journal = {Neural Computation},
	author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	month = dec,
	year = {1989},
	pages = {541--551},
	file = {LeCun et al. - 1989 - Backpropagation Applied to Handwritten Zip Code Re.pdf:/home/timo/Zotero/storage/674GVNFF/LeCun et al. - 1989 - Backpropagation Applied to Handwritten Zip Code Re.pdf:application/pdf},
}

@article{rosenblattPerceptronProbabilisticModel1958,
	title = {The perceptron: {A} probabilistic model for information storage and organization in the brain.},
	volume = {65},
	issn = {1939-1471, 0033-295X},
	shorttitle = {The perceptron},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0042519},
	doi = {10.1037/h0042519},
	language = {en},
	number = {6},
	urldate = {2021-07-08},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	pages = {386--408},
	file = {Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf:/home/timo/Zotero/storage/KK2R6SDH/Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf:application/pdf},
}

@article{nairRectifiedLinearUnits,
	title = {Rectified {Linear} {Units} {Improve} {Restricted} {Boltzmann} {Machines}},
	abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
	language = {en},
	author = {Nair, Vinod and Hinton, Geoffrey E},
	pages = {8},
	file = {Nair and Hinton - Rectified Linear Units Improve Restricted Boltzman.pdf:/home/timo/Zotero/storage/6KCH7VUJ/Nair and Hinton - Rectified Linear Units Improve Restricted Boltzman.pdf:application/pdf},
}

@article{bahdanauNeuralMachineTranslation2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a ﬁxed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a ﬁxed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	language = {en},
	urldate = {2021-07-08},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv: 1409.0473},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:/home/timo/Zotero/storage/AKDK7W3Q/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf},
}

@article{krizhevskyImageNetClassificationDeep2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2021-07-08},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:/home/timo/Zotero/storage/7MNWYXIX/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf},
}

@book{bishopPatternRecognitionMachine2006,
	address = {New York},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Machine learning, Pattern perception},
	file = {Bishop - 2006 - Pattern recognition and machine learning.pdf:/home/timo/Zotero/storage/MANSXQFX/Bishop - 2006 - Pattern recognition and machine learning.pdf:application/pdf},
}

@article{dosovitskiyIMAGEWORTH16X162021a,
	title = {{AN} {IMAGE} {IS} {WORTH} {16X16} {WORDS}: {TRANSFORMERS} {FOR} {IMAGE} {RECOGNITION} {AT} {SCALE}},
	language = {en},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	year = {2021},
	pages = {21},
	file = {Dosovitskiy et al. - 2021 - AN IMAGE IS WORTH 16X16 WORDS TRANSFORMERS FOR IM.pdf:/home/timo/Zotero/storage/9P7HMN8J/Dosovitskiy et al. - 2021 - AN IMAGE IS WORTH 16X16 WORDS TRANSFORMERS FOR IM.pdf:application/pdf},
}

@article{vaswaniAttentionAllYou2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2021-07-12},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Attention.pdf:/home/timo/Zotero/storage/KG99WGQU/Attention.pdf:application/pdf;Vaswani et al. - 2017 - Attention Is All You Need.pdf:/home/timo/Zotero/storage/UN6FY34C/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf},
}

@article{boutonReinforcementLearningIterative2020,
	title = {Reinforcement {Learning} with {Iterative} {Reasoning} for {Merging} in {Dense} {Traffic}},
	url = {http://arxiv.org/abs/2005.11895},
	abstract = {Maneuvering in dense trafﬁc is a challenging task for autonomous vehicles because it requires reasoning about the stochastic behaviors of many other participants. In addition, the agent must achieve the maneuver within a limited time and distance. In this work, we propose a combination of reinforcement learning and game theory to learn merging behaviors. We design a training curriculum for a reinforcement learning agent using the concept of level-k behavior. This approach exposes the agent to a broad variety of behaviors during training, which promotes learning policies that are robust to model discrepancies. We show that our approach learns more efﬁcient policies than traditional training methods.},
	language = {en},
	urldate = {2021-07-13},
	journal = {arXiv:2005.11895 [cs]},
	author = {Bouton, Maxime and Nakhaei, Alireza and Isele, David and Fujimura, Kikuo and Kochenderfer, Mykel J.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.11895},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {Bouton et al. - 2020 - Reinforcement Learning with Iterative Reasoning fo.pdf:/home/timo/Zotero/storage/QDRXXWU4/Bouton et al. - 2020 - Reinforcement Learning with Iterative Reasoning fo.pdf:application/pdf},
}

@inproceedings{yurtseverIntegratingDeepReinforcement2020,
	title = {Integrating {Deep} {Reinforcement} {Learning} with {Model}-based {Path} {Planners} for {Automated} {Driving}},
	doi = {10.1109/IV47402.2020.9304735},
	abstract = {Automated driving in urban settings is challenging. Human participant behavior is difficult to model, and conventional, rule-based Automated Driving Systems (ADSs) tend to fail when they face unmodeled dynamics. On the other hand, the more recent, end-to-end Deep Reinforcement Learning (DRL) based model-free ADSs have shown promising results. However, pure learning-based approaches lack the hard-coded safety measures of model-based controllers. Here we propose a hybrid approach for integrating a path planning pipe into a vision based DRL framework to alleviate the shortcomings of both worlds. In summary, the DRL agent is trained to follow the path planner's waypoints as close as possible. The agent learns this policy by interacting with the environment. The reward function contains two major terms: the penalty of straying away from the path planner and the penalty of having a collision. The latter has precedence in the form of having a significantly greater numerical value. Experimental results show that the proposed method can plan its path and navigate between randomly chosen origin-destination points in CARLA, a dynamic urban simulation environment. Our code is open-source and available online.},
	booktitle = {2020 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Yurtsever, Ekim and Capito, Linda and Redmill, Keith and Ozgune, Umit},
	month = oct,
	year = {2020},
	note = {ISSN: 2642-7214},
	keywords = {Reinforcement learning, Cameras, Mathematical model, Open source software, Path planning, Safety, Training},
	pages = {1311--1316},
	file = {Yurtsever et al. - 2020 - Integrating Deep Reinforcement Learning with Model.pdf:/home/timo/Zotero/storage/DTRNN7VG/Yurtsever et al. - 2020 - Integrating Deep Reinforcement Learning with Model.pdf:application/pdf;IEEE Xplore Abstract Record:/home/timo/Zotero/storage/UGJXHXAQ/9304735.html:text/html},
}

@article{liaoDecisionMakingStrategyHighway2020,
	title = {Decision-{Making} {Strategy} on {Highway} for {Autonomous} {Vehicles} {Using} {Deep} {Reinforcement} {Learning}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3022755},
	abstract = {Autonomous driving is a promising technology to reduce traffic accidents and improve driving efficiency. In this work, a deep reinforcement learning (DRL)-enabled decision-making policy is constructed for autonomous vehicles to address the overtaking behaviors on the highway. First, a highway driving environment is founded, wherein the ego vehicle aims to pass through the surrounding vehicles with an efficient and safe maneuver. A hierarchical control framework is presented to control these vehicles, which indicates the upper-level manages the driving decisions, and the lower-level cares about the supervision of vehicle speed and acceleration. Then, the particular DRL method named dueling deep Q-network (DDQN) algorithm is applied to derive the highway decision-making strategy. The exhaustive calculative procedures of deep Q-network and DDQN algorithms are discussed and compared. Finally, a series of estimation simulation experiments are conducted to evaluate the effectiveness of the proposed highway decision-making policy. The advantages of the proposed framework in convergence rate and control performance are illuminated. Simulation results reveal that the DDQN-based overtaking policy could accomplish highway driving tasks efficiently and safely.},
	journal = {IEEE Access},
	author = {Liao, Jiangdong and Liu, Teng and Tang, Xiaolin and Mu, Xingyu and Huang, Bing and Cao, Dongpu},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Machine learning, Mathematical model, Acceleration, Automobiles, Autonomous driving, Autonomous vehicles, Decision making, decision-making, deep Q-learning, deep reinforcement learning, dueling deep Q-network, overtaking policy, Road transportation},
	pages = {177804--177814},
	file = {Liao et al. - 2020 - Decision-Making Strategy on Highway for Autonomous.pdf:/home/timo/Zotero/storage/UXXT2E8H/Liao et al. - 2020 - Decision-Making Strategy on Highway for Autonomous.pdf:application/pdf;IEEE Xplore Abstract Record:/home/timo/Zotero/storage/Q9CKAS39/9190040.html:text/html},
}

@article{hamrickCOMBININGQLEARNINGSEARCH2020,
	title = {{COMBINING} {Q}-{LEARNING} {AND} {SEARCH} {WITH} {AMORTIZED} {VALUE} {ESTIMATES}},
	abstract = {We introduce “Search with Amortized Value Estimates” (SAVE), an approach for combining model-free Q-learning with model-based Monte-Carlo Tree Search (MCTS). In SAVE, a learned prior over state-action values is used to guide MCTS, which estimates an improved set of state-action values. The new Q-estimates are then used in combination with real experience to update the prior. This effectively amortizes the value computation performed by MCTS, resulting in a cooperative relationship between model-free learning and model-based search. SAVE can be implemented on top of any Q-learning agent with access to a model, which we demonstrate by incorporating it into agents that perform challenging physical reasoning tasks and Atari. SAVE consistently achieves higher rewards with fewer training steps, and—in contrast to typical model-based search approaches—yields strong performance with very small search budgets. By combining real experience with information computed during search, SAVE demonstrates that it is possible to improve on both the performance of model-free learning and the computational cost of planning.},
	language = {en},
	author = {Hamrick, Jessica B and Bapst, Victor and Pfaff, Tobias and Weber, Theophane and Battaglia, Peter W and Sanchez-Gonzalez, Alvaro and Buesing, Lars},
	year = {2020},
	pages = {27},
	file = {Hamrick et al. - 2020 - COMBINING Q-LEARNING AND SEARCH WITH AMORTIZED VAL.pdf:/home/timo/Zotero/storage/NFFGK9S3/Hamrick et al. - 2020 - COMBINING Q-LEARNING AND SEARCH WITH AMORTIZED VAL.pdf:application/pdf},
}

@article{petosaMultiplayerAlphaZero2019,
	title = {Multiplayer {AlphaZero}},
	url = {http://arxiv.org/abs/1910.13012},
	abstract = {The AlphaZero algorithm has achieved superhuman performance in two-player, deterministic, zero-sum games where perfect information of the game state is available. This success has been demonstrated in Chess, Shogi, and Go where learning occurs solely through self-play. Many real-world applications (e.g., equity trading) require the consideration of a multiplayer environment. In this work, we suggest novel modiﬁcations of the AlphaZero algorithm to support multiplayer environments, and evaluate the approach in two simple 3-player games. Our experiments show that multiplayer AlphaZero learns successfully and consistently outperforms a competing approach: Monte Carlo tree search. These results suggest that our modiﬁed AlphaZero can learn effective strategies in multiplayer game scenarios. Our work supports the use of AlphaZero in multiplayer games and suggests future research for more complex environments.},
	language = {en},
	urldate = {2021-07-14},
	journal = {arXiv:1910.13012 [cs]},
	author = {Petosa, Nick and Balch, Tucker},
	month = dec,
	year = {2019},
	note = {arXiv: 1910.13012},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Petosa and Balch - 2019 - Multiplayer AlphaZero.pdf:/home/timo/Zotero/storage/XL6HCNK7/Petosa and Balch - 2019 - Multiplayer AlphaZero.pdf:application/pdf;Petosa and Balch - 2019 - Multiplayer AlphaZero.pdf:/home/timo/Zotero/storage/RHMSUEAT/Petosa and Balch - 2019 - Multiplayer AlphaZero.pdf:application/pdf},
}

@inproceedings{lenzTacticalCooperativePlanning2016,
	title = {Tactical cooperative planning for autonomous highway driving using {Monte}-{Carlo} {Tree} {Search}},
	doi = {10.1109/IVS.2016.7535424},
	abstract = {Human drivers use nonverbal communication and anticipation of other drivers' actions to master conflicts occurring in everyday driving situations. Without a high penetration of vehicle-to-vehicle communication an autonomous vehicle has to have the possibility to understand intentions of others and share own intentions with the surrounding traffic participants. This paper proposes a cooperative combinatorial motion planning algorithm without the need for inter vehicle communication based on Monte Carlo Tree Search (MCTS). We motivate why MCTS is particularly suited for the autonomous driving domain. Furthermore, adoptions to the MCTS algorithm are presented as for example simultaneous decisions, the usage of the Intelligent Driver Model as microscopic traffic simulation, and a cooperative cost function. We further show simulation results of merging scenarios in highway-like situations to underline the cooperative nature of the approach.},
	booktitle = {2016 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Lenz, David and Kessler, Tobias and Knoll, Alois},
	month = jun,
	year = {2016},
	keywords = {Monte Carlo methods, Road transportation, Autonomous automobiles, Cost function, Merging, Planning, Vehicles},
	pages = {447--453},
	file = {Lenz et al. - 2016 - Tactical cooperative planning for autonomous highw.pdf:/home/timo/Zotero/storage/2EVNZPCC/Lenz et al. - 2016 - Tactical cooperative planning for autonomous highw.pdf:application/pdf;IEEE Xplore Abstract Record:/home/timo/Zotero/storage/8PWXD3ZI/7535424.html:text/html},
}

@article{chenDrivingManeuversPrediction2020,
	title = {Driving {Maneuvers} {Prediction} {Based} {Autonomous} {Driving} {Control} by {Deep} {Monte} {Carlo} {Tree} {Search}},
	volume = {69},
	issn = {1939-9359},
	doi = {10.1109/TVT.2020.2991584},
	abstract = {Autonomous driving has attracted significant attention in recent years. With the booming of artificial intelligence (AI), deep learning technologies have been applied to autonomous driving to help vehicles better perceive the environment. Besides the perceiving environment, predictive driving is another prominent smooth control and safe driving skill for human drivers. In this work, we develop a deep Monte Carlo Tree Search (deep-MCTS) control method for vision-based autonomous driving. Compared with existing deep learning-based autonomous driving control methods, our method can predict driving maneuvers to help improve the stability and performance of driving control. Two deep neural networks (DNNs) are employed for predicting action-state transformation and obtaining action-selection probabilities, respectively. The deep-MCTS utilizes the predicted information of the two DNNs and reconstructs multiple possible trajectories to predict driving maneuvers. An optimal trajectory is selected by the deep-MCTS based on both current road conditions and predicted driving maneuvers. The proposed method achieves high control stability by avoiding sharp turns and driving deviations. We implement our algorithm in the Udacity and Torcs self-driving environments. The testing results show that our algorithm achieves a significant improvement in training efficiency, the stability of steering control, and stability of driving trajectory compared to existing methods.},
	number = {7},
	journal = {IEEE Transactions on Vehicular Technology},
	author = {Chen, Jienan and Zhang, Cong and Luo, Jinting and Xie, Junfei and Wan, Yan},
	month = jul,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Vehicular Technology},
	keywords = {Artificial intelligence, Monte Carlo methods, Autonomous driving, Autonomous vehicles, Deep learning, Deep Monte Carlo Tree Search, Deep neural network, Neural networks, Sensors, Stability criteria, Trajectory},
	pages = {7146--7158},
	file = {Chen et al. - 2020 - Driving Maneuvers Prediction Based Autonomous Driv.pdf:/home/timo/Zotero/storage/S6TJ926G/Chen et al. - 2020 - Driving Maneuvers Prediction Based Autonomous Driv.pdf:application/pdf;IEEE Xplore Abstract Record:/home/timo/Zotero/storage/3QG8JHUZ/9082903.html:text/html},
}

@misc{raffinStableBaselines32019,
	title = {Stable baselines3},
	url = {https://github.com/DLR-RM/stable-baselines3},
	publisher = {GitHub},
	author = {Raffin, Antonin and Hill, Ashley and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Dormann, Noah},
	year = {2019},
}

@misc{guennebaudEigenV32010,
	title = {Eigen v3},
	url = {http://eigen.tuxfamily.org},
	author = {Guennebaud, Gaël and Jacob, Benoît and {others}},
	year = {2010},
}

@article{achiamSpinningDeepReinforcement2018,
	title = {Spinning up in deep reinforcement learning},
	author = {Achiam, Joshua},
	year = {2018},
}

@article{bishopMixtureDensityNetworks1994,
	title = {Mixture density networks},
	author = {Bishop, Christopher M},
	year = {1994},
	note = {Publisher: Aston University},
}

@book{goodfellowDeepLearning2016,
	title = {Deep learning},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@article{kullbackInformationSufficiency1951,
	title = {On information and sufficiency},
	volume = {22},
	number = {1},
	journal = {The annals of mathematical statistics},
	author = {Kullback, Solomon and Leibler, Richard A},
	year = {1951},
	note = {Publisher: JSTOR},
	pages = {79--86},
}

@misc{yaratsSoftActorcriticSAC2020,
	title = {Soft actor-critic ({SAC}) implementation in {PyTorch}},
	url = {https://github.com/denisyarats/pytorch_sac},
	publisher = {GitHub},
	author = {Yarats, Denis and Kostrikov, Ilya},
	year = {2020},
}

@article{silverRewardEnough2021,
	title = {Reward is enough},
	volume = {299},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370221000862},
	doi = {10.1016/j.artint.2021.103535},
	language = {en},
	urldate = {2021-07-16},
	journal = {Artificial Intelligence},
	author = {Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S.},
	month = oct,
	year = {2021},
	pages = {103535},
	file = {Silver et al. - 2021 - Reward is enough.pdf:/home/timo/Zotero/storage/B8KFLD8M/Silver et al. - 2021 - Reward is enough.pdf:application/pdf},
}

@article{moerlandThinkTooFast2020,
	title = {Think {Too} {Fast} {Nor} {Too} {Slow}: {The} {Computational} {Trade}-off {Between} {Planning} {And} {Reinforcement} {Learning}},
	shorttitle = {Think {Too} {Fast} {Nor} {Too} {Slow}},
	url = {http://arxiv.org/abs/2005.07404},
	abstract = {Planning and reinforcement learning are two key approaches to sequential decision making. Multistep approximate real-time dynamic programming, a recently successful algorithm class of which AlphaZero [Silver et al., 2018] is an example, combines both by nesting planning within a learning loop. However, the combination of planning and learning introduces a new question: how should we balance time spend on planning, learning and acting? The importance of this trade-off has not been explicitly studied before. We show that it is actually of key importance, with computational results indicating that we should neither plan too long nor too short. Conceptually, we identify a new spectrum of planning-learning algorithms which ranges from exhaustive search (long planning) to modelfree RL (no planning), with optimal performance achieved midway.},
	language = {en},
	urldate = {2021-07-16},
	journal = {arXiv:2005.07404 [cs]},
	author = {Moerland, Thomas M. and Deichler, Anna and Baldi, Simone and Broekens, Joost and Jonker, Catholijn M.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.07404},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Moerland et al. - 2020 - Think Too Fast Nor Too Slow The Computational Tra.pdf:/home/timo/Zotero/storage/KZA7DSCM/Moerland et al. - 2020 - Think Too Fast Nor Too Slow The Computational Tra.pdf:application/pdf},
}

@article{bellemareAutonomousNavigationStratospheric2020,
	title = {Autonomous navigation of stratospheric balloons using reinforcement learning},
	volume = {588},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-020-2939-8},
	doi = {10.1038/s41586-020-2939-8},
	language = {en},
	number = {7836},
	urldate = {2021-07-21},
	journal = {Nature},
	author = {Bellemare, Marc G. and Candido, Salvatore and Castro, Pablo Samuel and Gong, Jun and Machado, Marlos C. and Moitra, Subhodeep and Ponda, Sameera S. and Wang, Ziyu},
	month = dec,
	year = {2020},
	pages = {77--82},
	file = {Bellemare et al. - 2020 - Autonomous navigation of stratospheric balloons us.pdf:/home/timo/Zotero/storage/YYC94KVR/Bellemare et al. - 2020 - Autonomous navigation of stratospheric balloons us.pdf:application/pdf},
}

@article{weisbergAppliedLinearRegression,
	title = {Applied {Linear} {Regression}},
	language = {en},
	author = {Weisberg, Sanford},
	pages = {370},
	file = {Weisberg - Applied Linear Regression.pdf:/home/timo/Zotero/storage/P8JUD5TL/Weisberg - Applied Linear Regression.pdf:application/pdf},
}

@inproceedings{wangAlternativeLossFunctions2019,
	address = {Xiamen, China},
	title = {Alternative {Loss} {Functions} in {AlphaZero}-like {Self}-play},
	isbn = {978-1-72812-485-8},
	url = {https://ieeexplore.ieee.org/document/9002814/},
	doi = {10.1109/SSCI44817.2019.9002814},
	abstract = {Recently, AlphaZero has achieved outstanding performance in playing Go, Chess, and Shogi. Players in AlphaZero consist of a combination of Monte Carlo Tree Search and a deep neural network, that is trained using self-play. The uniﬁed deep neural network has a policy-head and a value-head, and during training, the optimizer minimizes the sum of policy loss and value loss. However, it is not clear if and under which circumstances other formulations of the loss function are better. Therefore, we perform experiments with different combinations of these two minimization targets. In contrast to many recent papers who adopt single run experiments and use the whole history Elo ratings from self-play, we propose to use repeated runs. The results show that this method can describe the training performance quite well within each training run, but there is a high self-play bias, such that it is incomparable among different training runs. Therefore, inspired by the AlphaGo series papers, a self-play bias avoiding performance assessment, ﬁnal best player Elo rating, is adopted to evaluate the playing strength in a direct competition between the evolved players. For relatively small games, based on this new evaluation method, surprisingly, minimizing only value loss achieves the strongest playing strength in the ﬁnal best players’ round-robin tournament. These results indicate that more research is needed into the relative importance of value function and policy function in small games.},
	language = {en},
	urldate = {2021-07-29},
	booktitle = {2019 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	publisher = {IEEE},
	author = {Wang, Hui and Emmerich, Michael and Preuss, Mike and Plaat, Aske},
	month = dec,
	year = {2019},
	pages = {155--162},
	file = {Wang et al. - 2019 - Alternative Loss Functions in AlphaZero-like Self-.pdf:/home/timo/Zotero/storage/6S34HLIC/Wang et al. - 2019 - Alternative Loss Functions in AlphaZero-like Self-.pdf:application/pdf},
}

@article{rusuPolicyDistillation2016,
	title = {Policy {Distillation}},
	url = {http://arxiv.org/abs/1511.06295},
	abstract = {Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-speciﬁc) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efﬁcient. Furthermore, the same method can be used to consolidate multiple task-speciﬁc policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.},
	language = {en},
	urldate = {2021-07-29},
	journal = {arXiv:1511.06295 [cs]},
	author = {Rusu, Andrei A. and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
	month = jan,
	year = {2016},
	note = {arXiv: 1511.06295},
	keywords = {Computer Science - Machine Learning},
	file = {Rusu et al. - 2016 - Policy Distillation.pdf:/home/timo/Zotero/storage/Q8P5KGVX/Rusu et al. - 2016 - Policy Distillation.pdf:application/pdf},
}

@article{kurzerParallelizationMonteCarlo2020,
	title = {Parallelization of {Monte} {Carlo} {Tree} {Search} in {Continuous} {Domains}},
	url = {http://arxiv.org/abs/2003.13741},
	abstract = {Monte Carlo Tree Search (MCTS) has proven to be capable of solving challenging tasks in domains such as Go, chess and Atari. Previous research has developed parallel versions of MCTS, exploiting today’s multiprocessing architectures. These studies focused on versions of MCTS for the discrete case. Our work builds upon existing parallelization strategies and extends them to continuous domains. In particular, leaf parallelization and root parallelization are studied and two ﬁnal selection strategies that are required to handle continuous states in root parallelization are proposed. The evaluation of the resulting parallelized continuous MCTS is conducted using a challenging cooperative multi-agent system trajectory planning task in the domain of automated vehicles.},
	language = {en},
	urldate = {2021-07-30},
	journal = {arXiv:2003.13741 [cs, stat]},
	author = {Kurzer, Karl and Hörtnagl, Christoph and Zöllner, J. Marius},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.13741},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Kurzer et al. - 2020 - Parallelization of Monte Carlo Tree Search in Cont.pdf:/home/timo/Zotero/storage/HILVHALQ/Kurzer et al. - 2020 - Parallelization of Monte Carlo Tree Search in Cont.pdf:application/pdf},
}

@misc{nvidiaNVIDIATensorRT2016,
	title = {{NVIDIA} {TensorRT}},
	url = {https://developer.nvidia.com/tensorrt},
	abstract = {NVIDIA TensorRT NVIDIA® TensorRT™ is an SDK for high-performance deep learning inference. It includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for deep learning inference applications. Get Started TensorRT-based applications perform up to 40X faster than CPU-only platforms during inference. With TensorRT, you can optimize neural network models trained in all major frameworks, calibrate for lower precision with high accuracy, and deploy to hyperscale data centers, embedded, or automotive product platforms.},
	language = {en},
	urldate = {2021-07-30},
	journal = {NVIDIA Developer},
	author = {NVIDIA},
	month = apr,
	year = {2016},
	file = {Snapshot:/home/timo/Zotero/storage/5V9PQKRA/tensorrt.html:text/html},
}
