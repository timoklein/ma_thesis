\glsresetall
\section{Discussion}\label{sec:discussion}
The previous chapter has given detailed results on the empirical performance of the proposed approach. These are interpreted in the following section. Next, the limitations of this thesis are discussed before an outlook provides paths for future research.

\subsection{Findings}\label{ssec:interpretation}
Reflecting the experimental results in the previous chapter, it stands out that the baseline and the guided search perform well in different settings. Scenario 06 is easily solvable by pre-calculated basic actions as evidenced by the strong \gls{mcts} performance with just five iterations (refer to Table~\ref{tab:baseline_performance}). On the other hand, scenario 08 seems to require more nuanced interaction between the agents and driving between two lanes. Here the guided search provides real benefits and can even realize a speed-up in wall clock time.

Looking closer at the findings of Section~\ref{ssec:eval_vs_mcts}, it emerges that the attained cooperative reward has no consistent relationship with the success rate. This is surprising, as both crashes and driving off the road are penalized heavily (see Appendix~\ref{app:reward_params}). It appears as if there are situations in which an agent may prefer to terminate the episode early instead of trying to resolve the situation.

Contrary to the usual choice of selection based on visitation counts \cite{silverMasteringGameGo2017, moerlandA0CAlphaZero2018}, the proposed approach performs better when selecting the action with the highest action value. This is in line with the baseline \gls{mcts}. One possible hypothesis explaining these results is that selection using the maximum action value is optimistic. Such behavior is problematic in competitive settings, where it can lead to exploitable policies. However, the tasks in this thesis are cooperative in nature. Thus agents have an incentive to accommodate behavior of other agents instead of exploiting it.

Overall, the training process of the proposed approach is remarkably stable, which is not necessarily a given in \gls{drl} methods \cite{hendersonDeepReinforcementLearning2019}. Various techniques which suggest improved final performance like using \gls{mcts} rollouts, centralized value targets or iteration randomization\footnote{See Appendix~\ref{app:iteration_randomization} for a short treatise of iteration randomization.} neither increase nor decrease the success rate substantially. Only using a \gls{gmm} instead of a normally distributed policy leads to noticeable success rate gains as analyzed by Section~\ref{ssec:eval_of_components}. This seems plausible given the motivation for using mixture models from Chapter~\ref{sssec:mixtures}.

Since the modifications described as having no effect in the previous paragraph all relate to value targets, it comes as no surprise that the learning is driven by the policy loss. Models trained using only the value loss show no learning progress at all. Yet this contradicts findings in the literature, where the value component of the objective function was found to be more important \cite{wangAlternativeLossFunctions2019}. Of course, there are significant differences in the environments: Board games have discrete action spaces compared to the continuous control scenarios introduced in Chapter~\ref{ssec:environment}. Additionally, domains like Go require building deeper trees to obtain a sparse reward rather than the wide trees with dense reward produced by the environments in this thesis. In conclusion, the experiment reveals that guiding the \gls{mcts} using a learned distribution is the important component of the model. Utilizing network value estimates has no further benefit.

Evaluating the generalization performance of trained networks shows higher success rates on unseen scenarios compared to a random model. The gains are more pronounced if the unseen task is similar to the scenario which is used during training. A network trained on scenario 06 for instance generalizes well to scenario 05, which has a setup closely resembling scenario 06. For reference, all scenarios are visualized in Appendix~\ref{app:scenarios}. An exciting result relates to training on multiple scenarios, which leads to improved performance on unseen tasks in Table~\ref{tab:generalization_performance}.

Motivated by the success of the baseline, actions determined through a heuristic are added to the guided search. This combination results in the algorithm with the highest overall success rate. However, one must be careful to not add too many pre-calculated actions. Adding more than the three basic maneuvers outlined in Section~\ref{ssec:eval_of_generalization} reduces the average success rate again as it inhibits sampling from the network's proposal distribution.

Nevertheless, the approach proposed in this thesis does not come without limitations. These are discussed in the following section.

\subsection{Limitations}\label{ssec:limitations}
Regarding the limitations of this thesis, the computational demands stand out in particular. Models trained using more iterations often need more than a day of training time. This makes it challenging to scale up the algorithm and either include several scenarios or use more difficult tasks which require a higher number of training iterations. In supervised learning, the scale of the models and training process is what drives computational demands (e.g. in \cite{devlinBERTPretrainingDeep2019}). Compared to that, the algorithm introduced in the previous chapters is bottlenecked by training data generation using network inference. As this work is a proof of concept, the aforementioned inference is not optimized for throughput speed. This in turn limits the scale at which the system can be trained.

The high demands in terms of compute also have trickle down effects concerning specific algorithm choices. For instance, one reason why the guided search does not perform better on some scenarios might be the coarse resolution of the map. This could lead to vehicles unnecessarily colliding with obstacles. Enhancing map size however leads to a significant increase in training time. As an example, doubling the resolution requires approximately $2.5 \times$ more wall clock time.

In terms of real world deployment, the proposed approach is limited due to the slow inference speed of the network. To be competitive in terms of wall clock time, the guided search must use significantly less iterations than the baseline \gls{mcts}. While this is achieved in scenario 08, scenario 06 requires too many iterations for the proposed model to recover the performance of the baseline. As a consequence, the guided search is orders of magnitude slower for the same success rate.

Also related to the available resources is hyperparameter tuning. The approach described in this thesis has around $170$ parameters to optimize\footnote{The exact number depends on the settings. As an example, enabling $\varepsilon$ action selection requires setting additional parameters which determine the decay.}, not including the network architecture. Optimizing performance in such a huge search space is difficult. One can therefore assume with high confidence that the used parameters are suboptimal.

The generalization capabilities of the model introduced by this thesis are highly task-dependent. The network trained on scenario 08 for example generalizes to scenarios 01-04. On the contrary, the model trained on scenario 06 also gains performance in scenario 05. What determines generalization success on unseen tasks? One might first assume it is the difficulty of the scenario. Upon closer inspection however, scenarios 01-04 and 08 have two lanes while scenarios 05 and 06 have three lanes. It is thus a reasonable hypothesis to conclude that models are only able to transfer experience to tasks with a similar number of lanes. This would preclude the learning of truly general driving skills.

Expounding the findings of the previous paragraph, it is fair to question the generality of the obtained results. Due to computational constraints, ablation studies are only performed in two scenarios. But as discussed, performance may be task dependent. More studies are therefore needed to examine whether the results and ablation studies are translatable to other scenarios and settings.

Lastly, looking at the proposed approach it stands out that the \gls{mcts} requires knowledge of the environment's transition function. As discussed in Section~\ref{sec:related_work}, this assumption is restrictive and limits the applications of the algorithm. In addition, a learned forward dynamics model could also increase the generalization capacities of this work's method.

Many of the limitations discussed in the current section provide avenues for future research. The chapter therefore concludes with an outlook on some particularly fruitful directions.

\subsection{Outlook}\label{ssec:outlook}
Recapping the previous section, increasing the speed at which training data is generated emerges as a stream for future work. Here three orthogonal directions are equally viable: First it is possible to increase throughput via improved engineering. Techniques such as quantization in PyTorch \cite{paszkePyTorchImperativeStyle} or Nvidia's TensorRT \cite{nvidiaNVIDIATensorRT2016} are able to improve network inference speed on CPUs by factors of $2 \times$ up to $4 \times$.

Another possibility is improving the architecture of system parts. Distilling the learned network \cite{rusuPolicyDistillation2016} or using an additional smaller policy in parts of the \gls{mcts} \cite{lanMultiplePolicyValue} are some conceivable modifications. A large stream of current research in the deep learning community is made of more efficient Transformer networks: Models like the Linformer or Performer are able to better trade off speed versus accuracy and are therefore prime candidates to improve the network architecture of this work as well \cite{tayLongRangeArena2020}.

Lastly, a third direction consists of modifications to the \gls{mcts} itself. Parallelizing the search is a well established research direction \cite{chaslotParallelMonteCarloTree2008} and has already been extended to continuous action spaces for the baseline used in this thesis \cite{kurzerParallelizationMonteCarlo2020}. Recently, more sophisticated approaches have been developed and shown larger performance gains \cite{liuWATCHUNOBSERVEDSIMPLE2020}. \gls{mcts} parallelization in connection with guided search might however be non-trivial to implement efficiently. Another method is therefore to be more selective with network evaluations, for instance by evaluating only the root node instead of all expanded nodes \cite{kurzerAcceleratingCooperativePlanning2020}.

Raising the system's scalability through improved engineering or more performant individual components seems dull and uninspiring at first. After all, it symbolizes incremental progress instead of an ingenious breakthrough. However, simply increasing the scale of learned models has led to important advances in research, for instance with AlexNet \cite{krizhevskyImageNetClassificationDeep2017}. Phenomena emerging from large-scale training also power some of the biggest success stories in natural language processing \cite{brownLanguageModelsAre2020}.

A further pathway for future research is provided through algorithms which utilize a learned model. This could be by either performing tree search in a compact latent space \cite{schrittwieserMasteringAtariGo2020} or by combining \gls{rl} with a learned model in other ways \cite{hongModelbasedLookaheadReinforcement2019}. As exact knowledge of the environment's transition function is a restrictive assumption, using a learned model instead will provide routes for a more general application of the proposed approach.

Finishing up this chapter, the evaluation results in Section~\ref{ssec:eval_of_generalization} hint at another rewarding direction: the combination of learned networks together with other forms of knowledge. Clearly integrating a heuristic into the guided search presented by this thesis is only a minuscule step towards such a concept. In the future, one can however envision systems that reason more abstractly about the underlying causes or constraints of an environment.