\glsresetall
\section{Related work}
\label{sec:related_work}
\begin{table}[h]
      \centering
      \scalebox{0.8}{
      {\renewcommand{\arraystretch}{2}
      \begin{tabular}{|L{1cm} L{1cm} | L{0.2cm} L{5.5cm} L{0.2cm} L{5.5cm}|}\hline
         & & \multicolumn{4}{c|}{\textbf{Action space}} \\\cline{3-6}
         &  & \multicolumn{2}{c|}{Discrete action space} & \multicolumn{2}{c|}{Continuous action space} \\\hline
        \multirow{2}{*}{ \rotatebox[origin=c]{90}{\textbf{Number of agents}}} & \multicolumn{1}{|l|}{\parbox{1.4cm}{Single \par agent}} & \textbf{A \newline } & \multicolumn{1}{l|}{\parbox{6cm}{\vspace{0.2cm} \underline{Example algorithms}: \textit{AlphaGo, AlphaZero, MuZero, SAVE} \par \underline{Example studies}: \cite{silverMasteringGameGo2016, silverMasteringGameGo2017, silverGeneralReinforcementLearning2018, schrittwieserMasteringAtariGo2020, hamrickCOMBININGQLEARNINGSEARCH2020} \vspace{0.2cm}}}  & \textbf{B \newline} & \multicolumn{1}{l|}{\parbox{6cm}{\vspace{0.2cm}\underline{Example algorithms}: \textit{A0C, Continuous MuZero, Sampled MuZero} \par \underline{Example studies}: \cite{moerlandA0CAlphaZero2018, yangContinuousControlSearching2020, hubertLearningPlanningComplex2021} \vspace{0.2cm}}} \\\cline{2-6}
         & \multicolumn{1}{|l|}{\parbox{1.5cm}{Multi \par agent}} & \textbf{C \newline} & \multicolumn{1}{l|}{\parbox{6cm}{\vspace{0.2cm} \underline{Example algorithms}: \textit{Multiplayer AlphaZero} \par \underline{Example studies}: \cite{petosaMultiplayerAlphaZero2019} \vspace{0.2cm}}} & \textbf{D \newline} & \multicolumn{1}{l|}{\parbox{6cm}{ \vspace{0.2cm} \textbf{\underline{Example algorithms}: \textit{This work}} \par \textbf{\underline{Example studies}: This study} \vspace{0.2cm}}} \\\hline
      \end{tabular}
      }}
    \caption[Literature on RL and search.]{Overview over prior research combining \gls{rl} and \gls{mcts}. While the AlphaZero family of algorithms utilizes self-play during training, they only plan for a single agent at a time. Therefore they are considered single-agent algorithms in this overview.}
 \label{tab:prior_research}
\end{table}
AlphaGo and its monumental win in the show match versus Lee Sedol have received a tremendous amount of attention \cite{silverMasteringGameGo2016}. Naturally, a plethora of follow-up research continues to improve the original algorithm or extends it for use in other domains. Work by the same group for instance removes the need for human expert knowledge with AlphaGo Zero \cite{silverMasteringGameGo2017} and applies the combination of \gls{rl} and \gls{mcts} to other games \cite{silverGeneralReinforcementLearning2018}. This results in the \emph{AlphaZero} algorithm. Expert iteration is another extension to AlphaGo which has been developed independently of AlphaGo Zero \cite{anthonyThinkingFastSlow2017}. In discrete action spaces, using Q-Learning instead of policy gradients shows improved performance \cite{hamrickCOMBININGQLEARNINGSEARCH2020}. Among follow-up works, the \emph{MuZero} algorithm stands out since it removes the need for knowing the environment's transition dynamics. Instead, MuZero performs tree search in a latent-space model \cite{schrittwieserMasteringAtariGo2020}. As the exact knowledge of the transition function is a very restrictive assumption, using an approximate model allows application to domains such as Atari \cite{schrittwieserMasteringAtariGo2020}. An orthogonal direction for extensions is provided with Multiplayer AlphaZero, which successfully applies a modified algorithm to three-player games \cite{petosaMultiplayerAlphaZero2019}. Multiplayer AlphaZero differs from this thesis in two ways: It considers a competitive setting instead of a cooperative one and does not learn interactions between agents.

A further stream of research focuses on developing combinations of \gls{rl} and \gls{mcts} for continuous action spaces. A0C is the first of such works, extending AlphaZero to simple continuous control scenarios provided by OpenAI Gym \cite{brockmanOpenAIGym2016}. It forms the basis for this thesis. Similar extensions have then been applied to MuZero, resulting in successful applications to classical control and simulated robot manipulation/locomotion tasks \cite{yangContinuousControlSearching2020, hubertLearningPlanningComplex2021}.

\cite{hoelCombiningPlanningDeep2020} apply an AlphaZero-inspired algorithm to tactical decision making in autonomous driving. Compared to this work, they do not learn in a continuous action space. Five high-level actions are used instead which are subsequently mapped to continuous actions by a physics model. \cite{hoelCombiningPlanningDeep2020} also consider single-agent settings only and do not plan actions for other agents in a scenario\footnote{While other agents exist in their scenarios, they do not learn and are controlled by a driver model.}. A very similar approach is presented by \cite{chenDrivingManeuversPrediction2020}. Other works use model-free \gls{rl} \cite{liaoDecisionMakingStrategyHighway2020}, combine \gls{rl} with $A^*$ planning \cite{yurtseverIntegratingDeepReinforcement2020} or use \gls{rl} together with game-theoretic reasoning \cite{boutonReinforcementLearningIterative2020}. A \gls{marl} approach to learning driving behavior is proposed by \cite{bacchianiMicroscopicTrafficSimulation2019}, from which the hybrid input representation in this thesis is derived.

When a multi-agent setting is considered, most works rely on learning a single policy that is used by all agents via parameter-sharing \cite{bacchianiMicroscopicTrafficSimulation2019, petosaMultiplayerAlphaZero2019}. Some algorithms do consider scenarios with a flexible number of agents. Deep sets \cite{hugleDynamicInputDeep2019} or graph networks \cite{huegleDynamicInteractionAwareScene2019} are able to learn interactions between different entities in an environment. They do however only learn behavior for a single decision-making agent. Another approach is to use \glspl{rnn} to process trajectories from multiple agents \cite{everettCollisionAvoidancePedestrianRich2020}. While an \gls{rnn} is able to aggregate information from different entities, its inherent sequentiality implies that some agents have more information at their disposal than others.

Transformers \cite{vaswaniAttentionAllYou2017} have emerged as the dominant architecture for processing sequential input, with applications in natural language \cite{devlinBERTPretrainingDeep2019} and trajectory forecasting \cite{giuliariTransformerNetworksTrajectory2020}. \cite{wrightNeuralAttentionalArchitecturesDeep} have proposed an attention-based network architecture which is conceptually similar to this thesis. However, they do not use a planner to improve the policy and instead rely on model-free \gls{rl}.

Lastly, this thesis builds on an \gls{mcts} approach for cooperative decision making in autonomous driving \cite{kurzerDecentralizedCooperativePlanning2018}. Note that \cite{kurzerDecentralizedCooperativePlanning2018} plan in a continuous action space compared to previous approaches which just learn high-level primitives \cite{lenzTacticalCooperativePlanning2016}. Using a neural network to guide the \gls{mcts} has already been investigated \cite{kurzerAcceleratingCooperativePlanning2020}. In contrast to this thesis, they rely on supervised learning from expert trajectories instead of \gls{rl} to train the network. Their architecture also uses a \gls{mlp} and thus produces a fixed-size output that does not scale with the number of agents within a scenario.
